<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>chapter17</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>#Principles and pathology of orthodox statistics.</p>
<p><span class="math inline">\(\leftarrow\)</span> <a href="./index.html">Back to Chapters</a></p>
<h3 id="comments-on-17.1.">Comments on 17.1.</h3>
<p>Given a fixed prior and model, a Bayesean estimator (MAP, posterior mean, or minimizing some other posterior risk insetad of MSE, or of any type at all) is still an estimator - it is a number <span class="math inline">\(f(X)\)</span> produced from data <span class="math inline">\(X\)</span>. It still partitions the data manifold into parts on which it gives the same estimate. Thus this line of Jaynes’s reasoning does not seem to distinguish between frequentist (“orthodox”) and Bayesean procedures.</p>
<h3 id="comments-on-17.4">Comments on 17.4</h3>
<p>Short summary of derivation of Cramer-Rao bound 17.30: apply Cauchy-Schwarts to the “covarince dot-product” of the score and the estimator. Slightly longer summary is as follows.</p>
<p>The formula 17.29 just expresses the following: covariance is a dot product (positive definite bilinear form) on the space of zero-mean random variables with finite second moments; the “(Cauchy-Buniakowski-)Schwartz” inequality for this dot product aka <span class="math inline">\(u\cdot v \leq |u||v|\)</span> is the formula 17.29. Attempting to embed random variables into <span class="math inline">\(L^2\)</span> functional space lead to the slightly-awkward formulas 17.28.</p>
<p>The cool thing is that the score – the derivative (in the parameter) of the log-likeliehood is – a zero mean random variable, whose variance is the Fisher information, and whose covariance with an estimator <span class="math inline">\(\beta\)</span> is the derivative (in the parameter) of the estimator’s expectation. This is why we get that standard deviation of the estimator is bounded below by (1+derivative of bias) over root of the Fisher information.</p>
<p>The equality condition is that the estimator and the score are proportional: that is, that the parameter is the parameter of exponential family, aka the “maximal entropy” family.</p>
<h3 id="comments-on-17.5.1">Comments on 17.5.1</h3>
<p>Here is how Jaynes’s criticism of “rejection of null hypothesis” statistical method transfers to a criticism of mathematical technique of “proof by contradiction” (quote from 15.5.1 with italics for modifications)""</p>
<p>This is the kind of logic that underlies <em>all proofs by contradiction</em>. In order to argue for a <em>conclusion C</em>, one does it indirectly: invent a hypothesis <em>“not C”</em> that denies <em>C</em>, then argue against <em>“not C”</em> in a way that makes no reference to <em>C</em> at all (that is, using only <em>deductions based</em> on <em>“not C”</em>). To see how far this procedure takes us from elementary logic, suppose we decide that the <em>conclusion holds</em>; that is, we reject <em>“not C”</em>. Surely, we must also reject <em>reasoning</em> based on <em>“not C”</em>; but then what was the logical justification for <em>the deduction</em>? <em>Proof by contradiction</em> saws off its own limb.</p>
</body>
</html>
