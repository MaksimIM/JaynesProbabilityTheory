<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>chapter9</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="repetitive-experiments-probability-and-frequency">Repetitive experiments: probability and frequency</h1>
<p><span class="math inline">\(\leftarrow\)</span> <a href="./index.html">Back to Chapters</a></p>
<h2 id="formulas-9.19-to-9.22">Formulas 9.19 to 9.22</h2>
<p>We are trying to solve a recurrence relation 9.19</p>
<p><span class="math inline">\(M(n, G)=\sum_{j=1}^{m} M(n-1, G-g_j)\)</span></p>
<p>subject to the boundary condition</p>
<p><span class="math inline">\(M(0, G)=\delta(G, 0).\)</span></p>
<p>Divinely inspired (or, rather, inspired by the fact that this is a kind of discretized version of a linear first order partial differential equation; but see below for more comments on this) we pretend that there exists some <span class="math inline">\(h(\lambda)\)</span> such that <span class="math inline">\(M(0, -G)\)</span> is a Laplace transform of <span class="math inline">\(h(\lambda)\)</span>, i.e. </p>
<p><span class="math display">\[\delta(G, 0)=M(0, G)=\int d \lambda \; \exp\{\lambda G\} h(\lambda)\]</span></p>
<p>Of course this prompts the question of whether such an <span class="math inline">\(h\)</span> actually exists. We ignore this for now (but see below).</p>
<p>Then, we compute <span class="math inline">\(M(n, G)\)</span> by induction on <span class="math inline">\(n\)</span>. Let’s start with <span class="math inline">\(n=1\)</span> from <span class="math inline">\(n=0\)</span>:</p>
<p><span class="math display">\[M(1, G)= \sum_{j=1}^{m} M(0, G-g_j)=\]</span></p>
<p><span class="math display">\[\sum_{j=1}^{m}\int d \lambda \; \exp\{\lambda (G-g_i)\} h(\lambda)=\]</span></p>
<p><span class="math display">\[\int d \lambda \; \left(\sum_{j=1}^{m} \exp\{-\lambda g_j\}\right) \exp\{ \lambda (G-g_i)\} h(\lambda)=\]</span></p>
<p><span class="math display">\[\int d \lambda \; Z(\lambda) \exp\{\lambda G\} h(\lambda)\]</span></p>
<p>where <span class="math inline">\(Z(\lambda)=\sum_{j=1}^{m} \exp\{-\lambda g_j\}\)</span>.</p>
<p>Of course we then have in the same way</p>
<p><span class="math display">\[M(n,G)=\int d \lambda \; Z^n(\lambda) \exp\{\lambda G\} h(\lambda)\]</span></p>
<p>aka 9.22.</p>
<p><strong>Remark 1, inspiration</strong>: This is an idea as old as Fourier. IF the initial condition was <span class="math inline">\(M(0, G)=\exp\{\lambda_0 G\}\)</span> for some fixed <span class="math inline">\(\lambda\)</span> then the fact that for such exponential functions shifting input <span class="math inline">\(G\)</span> to <span class="math inline">\(G-g_j\)</span> simply multiplies the result by <span class="math inline">\(\exp\{-\lambda_0 g_j\}\)</span>, we would have by induction <span class="math inline">\(M(n, G)=Z^n(\lambda_0) M(0, G)\)</span>. In other words, the function <span class="math inline">\(\exp\{\lambda_0 G\}\)</span> is an eigenfunction of the operator <span class="math inline">\(M(\cdot)\to \sum_{j=1}^{m} M(\cdot-g_j)\)</span> with eigenvalue <span class="math inline">\(Z(\lambda_0)\)</span>, so repeated application of this operator to increase <span class="math inline">\(n\)</span> just keeps multiplying the function by <span class="math inline">\(Z(\lambda_0)\)</span>. This is the content of the formulas 9.20 and 9.21: setting <span class="math inline">\(n=0\)</span> in 9.20 gives the boundary value <span class="math inline">\(M(0, G)=\exp\{\lambda_0 G\}\)</span>, and then the eigenvalue <span class="math inline">\(\exp\{\alpha\}\)</span> has to be equal to <span class="math inline">\(Z(\lambda_0)\)</span> .</p>
<p>Now the equation 9.19 is linear. So if we can make the initial condition <span class="math inline">\(M(0,n)\)</span> a sum of exponentials, the solution is the sum of corresponding solutions. This is what we did above.</p>
<p>This is analogous to how any exponential function <span class="math inline">\(\exp\{\lambda_0 x\}\)</span> being boundary condition to heat equation (at <span class="math inline">\(t=0\)</span>) leads to <span class="math inline">\(F(t,x)=\exp\{\lambda_0^2 t\}\exp\{\lambda_0 x\}\)</span> solution. In that setting, if one can write <span class="math inline">\(F(0, x)\)</span> as sum of exponentials then the solution follows. The heat equation is second order, so one <em>can</em> use sines and cosines as boundary conditions instead, and then decompose the actual boundary values into those (aka Fouries analysis). We have a first order difference equation, so we use exponentials.</p>
<p><strong>Remark 2, finding <span class="math inline">\(h\)</span></strong>: I don’t think there is such an <span class="math inline">\(h(\lambda)\)</span> (but have not proved this).</p>
<p><strong>Remark 3</strong>: I don’t think any of this matters. As far as I can tell, we are only going to be using 9.27, and not 9.22. That said, Jaynes’s derivation of 9.27 is suspect (and the talk of “inverse Laplace transform” seems doubly suspect; this would be right if we were doing Fourier transforms, but <a href="https://en.wikipedia.org/wiki/Inverse_Laplace_transform">not for Laplace</a>). So we better derive 9.27. We do this after an unnecessary detour through the 1D linear recurrences.</p>
<h4 id="bonus-1d-case-for-no-good-reason-maybe-for-motivation.">Bonus: 1D case for no good reason (maybe for motivation).</h4>
<p>The 9.19 is a 2-variable version of <a href="https://en.wikipedia.org/wiki/Recurrence_relation">recurrence relation</a>, more particularly of <a href="https://en.wikipedia.org/wiki/Constant-recursive_sequence">a constant-recursive sequence</a>,</p>
<p><span class="math inline">\(M(n)=\sum_{k=1}^d c_k M(n-k)\)</span></p>
<p>or putting <span class="math inline">\(c_0=-1\)</span>:</p>
<p><span class="math inline">\(\sum_{k=0}^d c_k M(n-k)=0\)</span></p>
<p>The answer is that if <span class="math inline">\(P(x)=\sum_{k=0}^{d} c_k x^k\)</span> has <strong>distinct</strong> roots <span class="math inline">\(\mu_j\)</span>, then <span class="math inline">\(M(n)\)</span> has the form <span class="math inline">\(\sum_{j=0}^{d} k_j \mu_j^n\)</span>. This is perhaps reminiscent of 9.20 (bearing in mind <span class="math inline">\(\mu_j^n=\exp\{ \alpha_j n \}\)</span> where <span class="math inline">\(\alpha_j=\log \mu_j\)</span>, with the caveat that <span class="math inline">\(\mu_j\)</span> are complex and could be 0).</p>
<p>One common approach to such equations is to study a <em>generating function</em> of the <span class="math inline">\(M(n)\)</span>, i.e.</p>
<p><span class="math display">\[f_0(x):=\sum_{n\geq 0} M(n) x^n\]</span></p>
<p>One sees that – modulo the edge effects – shifting the sequence to <span class="math inline">\(M(n-k)\)</span> just multiplies <span class="math inline">\(f_0\)</span> by <span class="math inline">\(x^{k}\)</span>:</p>
<p><span class="math display">\[f_k(x):=\sum_{n\geq k} M(n-k) x^n=\\
\sum_{n-k \geq 0 } M(n-k) x^{n-k} x^k=x^kf_0(x)\]</span></p>
<p>Then multiplying this by <span class="math inline">\(c_k\)</span> and summing over <span class="math inline">\(k=1, \ldots d\)</span> one sees that all the terms of degree <span class="math inline">\(\geq d\)</span> cancel, so that</p>
<p><span class="math inline">\(\sum_{k=0}^{d} c_k x^k f_0(x)=Q(x)\)</span></p>
<p>where <span class="math inline">\(Q(x)\)</span> is a polynomial of degree of at most <span class="math inline">\(d-1\)</span> consisting of all the terms that don’t cancel. Note that the <span class="math inline">\(Q(x)\)</span> will vary depending on the initial values <span class="math inline">\(M(0), M(1), \ldots M(d-1)\)</span>. Then</p>
<p><span class="math inline">\(f_0(x)=\frac{Q(x)}{\sum_{k=0}^{d} c_k x^k}\)</span></p>
<p>Then one can factorize the polynomial <span class="math inline">\(P(x)=\sum_{k=0}^{d} c_k x^k\)</span> and use partial fraction expansion of <span class="math inline">\(\frac{Q(x)}{P(x)}\)</span> to get a formula fo the coefficients <span class="math inline">\(M(n)\)</span>.</p>
<p>We want to make the following remarks about this procedure:</p>
<ol type="1">
<li><p>Upon substitution <span class="math inline">\(x=\frac{1}{z}\)</span> the generating function becomes <a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a>, a discrete time analog of Laplace transform (think <span class="math inline">\(z=\exp(s)\)</span> and <span class="math inline">\(\sum_{n\geq 0} M(n) z^{-n}\)</span> as <span class="math inline">\(\int_{t\geq 0} M(t) (\exp{s})^{-t})\)</span>.<br />
The “shifting the signal multiplies the transform” property that we used is the analogue of the corresponding property of Laplace transform. In fact the very recurrence we are solving is a discrete time analogue of an ODE. So we are using the “Laplace transform method for ODEs”, just in discrete time.</p></li>
<li><p>An advantage of working with generating functions is that they are <em>formal power series</em>, meaning we don’t worry about convergence.</p></li>
<li><p>From this perspective what we did is instead of solving for <span class="math inline">\(M(n)\)</span> directly, we solved for it’s Z/Laplace transform and then deduced what <span class="math inline">\(M(n)\)</span> must be.</p></li>
</ol>
<h2 id="getting-9.23-and-then-9.27.">Getting 9.23 (and then 9.27).</h2>
<p>The main point is to get 9.23. That is:</p>
<p><span class="math display">\[ \sum_G \exp\{-\lambda G\}M(n, G)=\sum_{(n_1, \ldots, n_m)} W(n_1, \ldots, n_m) \exp\{-\lambda \sum n_j g_j \}\]</span></p>
<p>with the sum over tuples of non-negative integers <span class="math inline">\((n_1, \ldots, n_m)\)</span> with <span class="math inline">\(\sum n_j=n\)</span> and</p>
<p><span class="math display">\[W(n_1, \ldots, n_m)=\frac{n!}{n_1! \cdot \ldots \cdot n_m!}.\]</span></p>
<p>This can be proven by induction on <span class="math inline">\(n\)</span>; we will do so below. But right now, we argue why it’s true directly: both sides simply sum over all possible sequences <span class="math inline">\(\vec{o}=(o_1,o_2, \ldots, o_n)\)</span> of length <span class="math inline">\(n\)</span> of outcomes <span class="math inline">\(o_t\in \{1, \ldots, m\}\)</span> the value</p>
<p><span class="math display">\[V(\vec{o})=\exp\{-\lambda \sum_{t=1}^n g_{o_t}\}.\]</span></p>
<p>The left hand side groups the outcome sequences <span class="math inline">\(\vec{o}\)</span> by their “total amount generated” i.e. by <span class="math inline">\(G=\sum g_{o_t}\)</span>. The number of sequences with this <span class="math inline">\(G\)</span> is by definition <span class="math inline">\(M(n, G)\)</span>, and each one contributes <span class="math inline">\(\exp\{-\lambda \sum_{t=1}^{m} g_{o_t} \}\)</span>, hence the left hand side.</p>
<p>The right hand side groups the outcome sequences by frequencies <span class="math inline">\(n_j\)</span> of the outcome <span class="math inline">\(j\in \{1, \ldots, m  \}\)</span>. For a fixed tuple <span class="math inline">\((n_1, \ldots, n_m)\)</span> of frequencies there are <span class="math inline">\(W(n_1, \ldots, n_m)\)</span> sequences <span class="math inline">\(\vec{o}\)</span>, each generating <span class="math inline">\(\exp\{-\lambda \sum n_j g_j \}\)</span>, hence the right hand side.</p>
<p>Thus 9.23 follows. The passage from 9.23 to 9.27 is reasonably well explained in Jaynes.</p>
<h4 id="bonus-induction-proof-of-9.23">Bonus: induction proof of 9.23</h4>
<p>Base: The <span class="math inline">\(n=0\)</span> case is simply <span class="math inline">\(1=1\)</span>, which is true.</p>
<p>Step: Using 9.19 ( i.e. <span class="math inline">\(M(n, G)=\sum_{j=1}^{m} M(n-1, G-g_j)\)</span>) we have:</p>
<p><span class="math display">\[ \sum_G \exp\{-\lambda G\}M(n, G)=\sum_G \exp\{-\lambda G\} \left(\sum_{j=1}^{m} M(n-1, G-g_j)\right)\]</span></p>
<p>We massage this a bit to make it fit the formulas we have in the induction hypothesis (note that we use that all the sums are over finite sets to exchange order of summation as we see fit):</p>
<p><span class="math display">\[= \sum_G \sum_{j=1}^{m}    \exp\{-\lambda g_j\} \exp\{-\lambda (G-g_j)\} M(n-1, G-g_j)\]</span></p>
<p><span class="math display">\[= \sum_{j=1}^{m}\exp\{-\lambda g_j\}   \sum_G   \exp\{-\lambda (G-g_j)\} M(n-1, G-g_j)\]</span></p>
<p>Now by induction hypothesis (we don’t say this separately, but the summands where one of the elements of the tuple becomes negative are excluded from the corresponding sum):</p>
<p><span class="math display">\[ =\sum_{j=1}^{m}\exp\{-\lambda g_j\}   \sum_{(n_1, \ldots, n_j-1,\ldots, n_m)}  W(n_1, \ldots, n_j-1,\ldots, n_m) \exp\{-\lambda (\sum n_j g_j -g_j) \}  \]</span></p>
<p><span class="math display">\[ =\sum_{j=1}^{m}   \sum_{(n_1, \ldots, n_j-1, \ldots, n_m)}  W(n_1, \ldots, n_j-1,\ldots, n_m) \exp\{-\lambda (\sum n_j g_j) \}  \]</span></p>
<p>Now since <span class="math inline">\(W(n_1, \ldots, n_j-1,\ldots, n_m)=\frac{n_j}{n}W(n_1, \ldots, n_j,\ldots, n_m)\)</span> we get:</p>
<p><span class="math display">\[ =\sum_{j=1}^{m}   \sum_{(n_1, \ldots, n_j-1, \ldots, n_m)}  \frac{n_j}{n}W(n_1, \ldots, n_j,\ldots, n_m) \exp\{-\lambda (\sum n_j g_j) \}  \]</span></p>
<p><span class="math display">\[ = \sum_{(n_1, \ldots, n_j,\ldots, n_m)}  W(n_1, \ldots, n_j,\ldots, n_m) \exp\{-\lambda (\sum n_j g_j) \}  \]</span></p>
<h2 id="a-version-of-exercise-9.1">A version of Exercise 9.1</h2>
<p>For more on the “missing species” problem see <a href="https://en.wikipedia.org/wiki/Unseen_species_problem">Wikipedia</a>, the <a href="https://www.jstor.org/stable/1411">paper of Fisher</a> and Section 6.2 of the <a href="https://doi.org/10.1017/CBO9781316576533">Computer Age Statistical Inference</a> by Efron and Hastie.</p>
<p>We follow the above references answer a different, albeit related, question: namely, how many new species we may hope to find if we sample more i.e. collect more specimens.</p>
<p>According to Fisher,</p>
<p><span class="math inline">\(S=-\alpha \ln (1-x)\)</span>, <span class="math inline">\(N=\alpha \frac{x}{1-x}\)</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is some kind of “average Poisson rate” parameter and is independent of size of sample, while <span class="math inline">\(x=\frac{\sigma}{\sigma+1}\)</span> where <span class="math inline">\(\sigma=\frac{x}{1-x}\)</span> the scale parameter in the prior Gamma distribution over rates of Poissons, and grows linearly with sample size/sample time.</p>
<p>Then</p>
<p><span class="math inline">\(S=\alpha \ln(1+\sigma)\)</span>, <span class="math inline">\(N=\alpha \sigma\)</span>, <span class="math inline">\(\frac{S}{N}=\frac{\ln(1+\sigma)}{\sigma}\)</span>.</p>
<p>For <span class="math inline">\(N=122\)</span>, <span class="math inline">\(S=19\)</span> this gives <span class="math inline">\(\frac{\ln (1+t)}{t}=\frac{19}{122}\)</span></p>
<p>and</p>
<p><span class="math display">\[\sigma=19.34583762707454276246963164708576325676864924349274639666\]</span> <!---
 $$\alpha=6.306266099807471184873899660081665374943243555421222377981$$
 ---></p>
<p>This can be expected to be reasonably accurate up to about doubling the sample size/<span class="math inline">\(\sigma\)</span>.</p>
<p>After double the sample size i.e. doubling <span class="math inline">\(\sigma\)</span>, the expected number of species <span class="math inline">\(S\)</span> becomes</p>
<p><span class="math display">\[S_{new}=S_{init}\frac{\ln (1+2\sigma)}{\ln (1+\sigma)}=23.21425768843744049107893270266974263904162495024301691901\]</span></p>
<p>So after collecting <span class="math inline">\(122\)</span> more samples we expect to discover about <span class="math inline">\(4\)</span> new species.</p>
<h2 id="exercise-9.2">Exercise 9.2</h2>
<p>Presumably we are to rederive formula 3.89 (and no 3.77).</p>
<p>Given <span class="math inline">\(k\)</span> linear functions <span class="math inline">\(\mathcal{G}_1, \ldots \mathcal{G}_k\)</span> of <span class="math inline">\(n_j\)</span> with <span class="math inline">\(\mathcal{G}_l(n_1, \ldots, n_m)=\sum g^l_j n_j\)</span> we introduce <span class="math inline">\(M(n, G_1, \ldots, G_k)\)</span> as the number of tuples <span class="math inline">\(\vec{n}=(n_1, \ldots, n_m)\)</span> such that <span class="math inline">\(\mathcal{G}_l(\vec{n})=G_l\)</span>.</p>
<p>Recursion 9.19 becomes</p>
<p><span class="math display">\[M(n, G_1, \ldots, G_k)=\sum_{j=1}^m M(n-1, G_1-g^1_j,  \ldots, G_k-g^k_j)\]</span></p>
<p>Ansatz 9.20 becomes</p>
<p><span class="math display">\[M(n, G_1, \ldots, G_k)=\exp\{\alpha n +\sum_l \lambda_l G_l\} \]</span></p>
<p>and condition 9.21</p>
<p><span class="math display">\[\exp\{\alpha\}=Z(\lambda_1, \ldots, \lambda_k)=\sum_{j=1}^m \exp\{\sum_{l=1}^k -\lambda_l g^l_j\}\]</span></p>
<p>To shorten notation we write <span class="math inline">\(\vec{G}=(G_1, \ldots, G_k)\)</span> and <span class="math inline">\(\vec{\lambda}=(\lambda_1, \ldots, \lambda_k)\)</span>. Then general solution 9.22 becomes</p>
<p><span class="math display">\[H(n, \vec{G})=\int d \vec{\lambda}  \;\;Z^n(\vec{\lambda}) \exp\{\vec{\lambda} \cdot \vec{G}\}\,h(\vec{\lambda})\]</span></p>
<p>Then with initial condition <span class="math inline">\(M(0, \vec{G})=\delta(\vec{G},\vec{0})\)</span> we have as before</p>
<p><span class="math display">\[Z^n(\vec{\lambda})=\sum_{\vec{G}} M(n, \vec{G})\exp\{-\vec{\lambda} \cdot \vec{g}\} \]</span></p>
<p>Now we apply this to derive multinomial distribution. Partition outcomes <span class="math inline">\(n_j\)</span> into <span class="math inline">\(k\)</span> sets <span class="math inline">\(S_1, \ldots, S_k\)</span> of size <span class="math inline">\(s_1, \ldots,s_k\)</span> and let <span class="math inline">\(\mathcal{G}_l(n_j)=\begin{cases}1 \text{ if } n_j\in S_l\\0 \text{ else } \end{cases}\)</span></p>
<p>Then <span class="math inline">\(M(n, c_1, \ldots, c_k)\)</span> is the number of ways to get a tuple <span class="math inline">\(\vec{n}\)</span> with <span class="math inline">\(\sum_{j\in S_l} n_j=c_l\)</span> and we can compute</p>
<p><span class="math display">\[Z(\vec{\lambda})=\sum_l s_l \exp\{-\lambda_l\}=\sum_l s_l x_l\]</span></p>
<p><span class="math display">\[Z^n(\vec{\lambda})=\sum_{(c_1, \ldots c_k)} {n\choose c_1, \ldots c_k}\prod_l (s_l x_l)^{c_l}\]</span></p>
<p>and so</p>
<p><span class="math display">\[M(n, c_1, \ldots, c_k)= {n\choose c_1, \ldots c_k}\prod_l s_l^{c_l}\]</span></p>
<p>Probability of getting sequence of this type is <span class="math inline">\(\frac{M(n, c_1, \ldots, c_k)}{n^m}\)</span> and denoting <span class="math inline">\(\frac{s_l}{n}=f_l\)</span> we have</p>
<p><span class="math display">\[ P(c_1, \ldots, c_k)= {n\choose c_1, \ldots c_k}\prod_l f_l^{c_l}\]</span></p>
<p>in agreement with 3.89.</p>
<h2 id="comments-on-section-9.7">Comments on Section 9.7</h2>
<p>“If A is linear in the <span class="math inline">\(n_j\)</span>, then it is the same as our <span class="math inline">\(G\)</span> in (9.17)” – I think Jaynes means that “for a given linear function <span class="math inline">\(\mathcal{G}\)</span> of <span class="math inline">\(n_j\)</span>s, let <span class="math inline">\(A\)</span> be the proposition ‘<span class="math inline">\(\mathcal{G}\)</span> takes value <span class="math inline">\(G\)</span>’”. In that case, indeed, <span class="math inline">\(M(n, G)=M(n, A)\)</span> by definition.</p>
<p><!--
 -- for $n>1$ this is only possible if both are zero: a non-zero linear function would have to attain at least 3  values, unlike a characteristic one. One should probably consider a generalization of both where $A$ is an arbitrary function of $n_j$s -- neither "proposition", nor a linear function.
 ---></p>
<p>“the notion of entropy inherent in probability theory independently of the work of Shannon” – in the work of Shannon, the notion of entropy is derived from combinatorics of sequences, much like here.</p>
<h2 id="exercise-9.3-the-first-one">Exercise 9.3 (the first one)</h2>
<p><span class="math display">\[f&#39;&#39;_{k}=\frac{n_k-\delta_{jk}-\delta_{tk}}{n-2}\]</span></p>
<p><span class="math display">\[\delta f_k=f&#39;&#39;_k-f_k=f&#39;&#39;_k-\frac{n_k}{n}=\frac{2f_k-\delta_{jk}-\delta_{tk}}{n-2}\]</span></p>
<p>so</p>
<p><span class="math display">\[\delta H=\sum_k (-1-\ln(f_k))\frac{2f_k-\delta_{jk}-\delta_{tk}}{n-2}\]</span></p>
<p><span class="math display">\[\frac{2H+\ln(f_j)+\ln(f_t)}{n-2}\]</span></p>
<p>and</p>
<p><span class="math display">\[H&#39;&#39;=\frac{nH+\ln(f_j)+\ln(f_t)}{n-2}\]</span></p>
<p><span class="math display">\[M(n-2, G-g_j-g_t)=\exp\{(n-2)H&#39;&#39;\}=f_jf_t \exp\{nH\}\]</span></p>
<p>so</p>
<p><span class="math display">\[p(r_i=j, r_s=t|GnI_0)=f_jf_t\]</span></p>
<p>The trials are still pairwise independent (and by similar analysis <span class="math inline">\(l\)</span>-wise independent as long as <span class="math inline">\(k\ll n\)</span>).</p>
<h2 id="formula-9.78">Formula 9.78</h2>
<p>Assuming not all <span class="math inline">\(g_j\)</span> are the same, strict convexity of <span class="math inline">\(\ln Z\)</span> implies that the derivative <span class="math inline">\(-\frac{\partial \ln Z}{\partial \lambda}\)</span> attains each value in its range once. However, it only attains values <span class="math inline">\(\min g_j&lt;\lambda &lt;\max g_j\)</span>. Averages <span class="math inline">\(\bar{G}\)</span> outside this range - with exception of the endpoints <span class="math inline">\(\min g_j\)</span> and <span class="math inline">\(\max g_j\)</span> - are impossible (the endpoints correspond to <span class="math inline">\(\lambda=\pm \infty\)</span> and would require those <span class="math inline">\(u_i\)</span> with non-extremal <span class="math inline">\(g_i\)</span> to be set to zero, while not restricting the others; this is a rather special case, not directly covered by Jaynes’ analysis in this section). We better condition on an achievable average.</p>
</body>
</html>
