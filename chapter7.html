<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>chapter7</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="the-central-gaussian-or-normal-distribution">The central, Gaussian or normal distribution</h1>
<p><span class="math inline">\(\leftarrow\)</span> <a href="./index.html">Back to Chapters</a></p>
<h2 id="exercise-7.1">Exercise 7.1</h2>
<p>TO DO</p>
<!---

Possible interpretations: 

1) We literally take $\epsilon_n=\sum_1^n \varepsilon_i$ with $\varepsilon_i\sim \epsilon$ and i.i.d. Then $\langle \epsilon_n^2 \rangle=n \langle \epsilon^2\rangle$. 



2) We suppose $\epsilon=\sum_{i=1}^{n}\epsilon_i$ of i.i.d. random noises $\epsilon_i$, with $\epsilon_i$ therefore satisfying $\langle \epsilon_i \rangle=0$ and  $n\langle \epsilon_i^2 \rangle=\langle \epsilon^2 \rangle$. 

--->
<h2 id="exercise-7.2">Exercise 7.2</h2>
<p>Consider the family of distributions <span class="math inline">\(p_{\mu, \sigma}(v)\)</span>.</p>
<p>We want to express fact that convolution of <span class="math inline">\(p_{\mu, \sigma}(v)\)</span> with <span class="math inline">\(q(v)\)</span> still belongs to the same family. I will prefer to work with parameter <span class="math inline">\(\nu=\sigma^2\)</span> instead. To avoid confusion the variable <span class="math inline">\(v\)</span> will be replaced by <span class="math inline">\(x\)</span>. So we work with the family <span class="math inline">\(p_{\mu, \nu}(x)\)</span>.</p>
<p>Let <span class="math inline">\(\mu_q=\langle \epsilon\rangle_q\)</span> be the mean and <span class="math inline">\(\nu_q=\langle \epsilon^2\rangle_q-\langle \epsilon\rangle_q^2\)</span> the variance of <span class="math inline">\(q\)</span>. Then the new distribution must be <span class="math inline">\(p_{\mu+\mu_q, \nu+\nu_q}(x)\)</span>. At the same time the expansion 7.20 becomes</p>
<p><span class="math display">\[p_{\mu+\mu_q, \nu+\nu_q}(x)=p_{\mu, \nu}(x)-\mu_q \frac{\partial}{\partial x} p_{\mu, \nu}(x)+\frac{1}{2}(\nu_q+\mu_q^2)\frac{\partial^2}{\partial^2 x} p_{\mu, \nu}(x)+\ldots\]</span></p>
<p>Taylor expanding around <span class="math inline">\(\mu, \nu\)</span></p>
<p><span class="math display">\[p_{\mu+\mu_q, \nu+\nu_q}(x)= p_{\mu, \nu}(x)+\mu_q\frac{\partial}{\partial \mu}p_{\mu, \nu}(x)+ \nu_q\frac{\partial}{\partial \nu}p_{\mu, \nu}(x)+ \]</span></p>
<p><span class="math display">\[\frac{1}{2} \mu_q^2 \frac{\partial^2}{\partial^2 \mu}p_{\mu, \nu}(x) +\frac{1}{2}\nu_q^2 \frac{\partial^2}{\partial^2 \nu}p_{\mu, \nu}(x)+  \mu_q \nu_q \frac{\partial^2}{\partial \mu \partial \nu} p_{\mu, \nu}(x)\]</span></p>
<p><span class="math display">\[ \]</span></p>
<p>Now <strong>if</strong> we wanted this to be true for arbitrary (small) <span class="math inline">\(\mu_q, \nu_q\)</span> we would have equality of Taylor coefficients:</p>
<p><span class="math display">\[-\frac{\partial}{\partial x}p_{\mu, \nu}(x)=\frac{\partial}{\partial \mu}p_{\mu, \nu}(x)\]</span></p>
<p><span class="math display">\[\frac{1}{2}\frac{\partial^2}{\partial^2 x} p_{\mu, \nu}(x)=\frac{\partial}{\partial \nu}p_{\mu, \nu}(x)= \frac{1}{2} \frac{\partial^2}{\partial^2 \mu}p_{\mu, \nu}(x)\]</span></p>
<p>where the first equation says that <span class="math inline">\(p_{\mu, \nu}(x)\)</span> is a function of <span class="math inline">\(x-\mu\)</span> and not of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(x\)</span> separately, <span class="math inline">\(p_{\mu, \nu}(x)=f_\nu(x-\mu)\)</span>. From this <span class="math inline">\(\frac{\partial^2}{\partial^2 x} p_{\mu, \nu}(x)=\frac{\partial^2}{\partial^2 \mu} p_{\mu, \nu}(x)\)</span> follows, and we simply recover the more general Gaussina family <span class="math inline">\(p_{\mu,\nu}(x)=\frac{1}{\sqrt{2 \pi \nu}} \exp\{-\frac{(x-\mu)^2}{2 \nu}\}\)</span> as in 7.23.</p>
<p>However, <strong>if</strong> we instead think of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span> as <span class="math inline">\(\mu(t)\)</span> and <span class="math inline">\(\nu(t)\)</span> so that the family <span class="math inline">\(p_t(x)=p_{\mu(t), \nu(t)}(x)\)</span> is a single-parameter family, then the expansions become expansions in terms of <span class="math inline">\(t\)</span>: with <span class="math inline">\(\mu_q(t)=\mu_q&#39;(0)t+o(t^2)\)</span>, <span class="math inline">\(\nu_q(t)=\nu_q&#39;(0)t+o(t^2)\)</span></p>
<p><span class="math display">\[p_{\mu+\mu_q(t), \nu+\nu_q(t)}(x)=p_{\mu, \nu}(x)+[-\mu_q&#39;(0) \frac{\partial}{\partial x} p_{\mu, \nu}(x)+\frac{1}{2}\nu_q&#39;(0)\frac{\partial^2}{\partial^2 x} p_{\mu, \nu}(x)]t+\]</span></p>
<p><span class="math display">\[p_{\mu+\mu_q(t), \nu+\nu_q(t)}(x)=p_{\mu, \nu}(x)+\frac{\partial}{\partial t}p_{\mu, \nu}(x) t+o(t^2)\]</span></p>
<!--
$$p_{\mu+\mu_q(t), \nu+\nu_q(t)}(x)= p_{\mu, \nu}(x)+[\mu_q'\frac{\partial}{\partial \mu}p_{\mu, \nu}(x)+ \nu_q'\frac{\partial}{\partial \nu}p_{\mu, \nu}(x)]t+ o(t^2)$$

--->
<p>Equating Taylor coefficients:</p>
<p><span class="math display">\[ \frac{\partial}{\partial t}p_{t}(x)= -\mu_q&#39; \frac{\partial}{\partial x} p_{t}(x)+\frac{1}{2}\nu_q&#39;\frac{\partial^2}{\partial^2 x} p_{t}(x)\]</span></p>
<p>This is a <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Plank equation</a>, albeit a very special one, with <span class="math inline">\(\mu(x,t)=\mu&#39;(0)\)</span>, <span class="math inline">\(\sigma^2(x,t)=\nu&#39;(0)\)</span>, corresponding to the stochastic process where the drift <span class="math inline">\(\mu\)</span> and diffusion coefficient <span class="math inline">\(\nu/2\)</span> are both constant. Denote <span class="math inline">\(\mu&#39;(0)=m\)</span> and <span class="math inline">\(\nu&#39;(0)=v\)</span>.</p>
<p>Changing coordinates to <span class="math inline">\(y(x,t)=x-mt\)</span> aka <span class="math inline">\(x(y,t)=y+mt\)</span>, we have <span class="math display">\[p_t(x(y,t))=p_t(y+mt)=:q_t(y)\]</span></p>
<p>and compute by chin rule</p>
<p><span class="math display">\[\frac{\partial}{\partial t}q_{t}(y)=\frac{\partial}{\partial t}p_{t}(x(y,t))=\frac{\partial}{\partial t}p_{t}(y+mt)+ m  \frac{\partial}{\partial x}p_{t}(y+mt)\]</span></p>
<p>while <span class="math display">\[\frac{1}{2}v\frac{\partial^2}{\partial^2 y} q_{t}(y)=\frac{1}{2}v\frac{\partial^2}{\partial^2 y} p_{t}(x(y,t))=\frac{1}{2}v\frac{\partial^2}{\partial^2 x} p_{t}(y+mt) \]</span></p>
<p>So the substitution we made reduces the Fokker-Plank equation we have (with drift) to the diffusion equation (without drift) i.e. 7.22 (with <span class="math inline">\(\sigma^2=t\)</span>), which by 7.23 has solution <span class="math inline">\(q_t(y)=\frac{1}{\sqrt{2\pi t }}\exp\{-\frac{y^2}{2t}\}\)</span>, or, after substitution</p>
<p><span class="math display">\[p_t(x)=\frac{1}{\sqrt{2\pi t }}\exp\{-\frac{(x-mt)^2}{2t}\}\]</span></p>
<p>This has variance <span class="math inline">\(\sigma^2=t\)</span> so we can rewrite it as <span class="math inline">\(p(x)=\frac{1}{\sqrt{2\pi \sigma^2 }}\exp\{-\frac{(x-m\sigma^2)^2}{2\sigma^2}\}\)</span>, as in the formulation of the exercise.</p>
<p><!---
 
 $$=\mu_q'\frac{\partial}{\partial \mu}p_{\mu, \nu}(x)+ \nu_q'\frac{\partial}{\partial \nu}p_{\mu, \nu}(x)$$
 
 ---></p>
<p>## Exercise 7.3 preliminary remarks.</p>
<p>I interpret this as follows. We are now considering evolution of beliefs about noise-generating process, and want to update this based on data and see that as the amount of data grows our posterior beliefs about the noise-generating process will imply beliefs about frequencies of noise that will converge to those produced by true frequencies <span class="math inline">\(f(e)\)</span>.</p>
<p>In order to talk about the above meaningfully in mathematical sense, one needs to define some space of noise-genera processes in such a way that formulating a prior probability over it is possible, and also define in what sense the posterior beliefs about frequencies “converge”.</p>
<p>Since we only observe real-valued noise data (and no other type of information about the noise-generating processes) the space of noise-generating processes can be taken to be the set of real-valued stochastic processes. Now this is still very large. Now one could either 1) take only noise-generating processes that generate noise in i.i.d. way 2) focus only on the beliefs about “frequencies” i.e. how often various values (close to) various <span class="math inline">\(e\)</span>s are observed - or both.</p>
<p>The intuition is that those processes that produce incorrect frequency of <span class="math inline">\(e\)</span>s will be suppressed in he update (due to a mechanism like that in the Borel’s law of large numbers), leaving only the process that do have the correct frequencies in the “posterior distribution over processes”, whatever that means. To illustrate some of the difficulties, consider the simple case of discrete time and processes that just i.i.d. sample from some probability distribution. Consider a distribution which moreover has a density <span class="math inline">\(p\)</span>. Then a particular dataset <span class="math inline">\(e_1, \ldots, e_n\)</span> has likelihood <span class="math inline">\(\prod_{i=1}^{N} p(e_i)\)</span>. At each finite <span class="math inline">\(N\)</span> the closer our distribution <span class="math inline">\(p\)</span> is to the empirical distribution(of <span class="math inline">\(\frac{1}{N}\sum_i \delta{e_i}\)</span>) the higher the likelihood; this seems problematic. Perhaps one has to discretize the set of possible <span class="math inline">\(e\)</span>s and then take limit, or simply use some more sophisticated analysis.</p>
<p>## Footnote 12</p>
<p>####General distributions</p>
<p><strong>Sum of weights.</strong></p>
<p>We are doing MLE for the location parameter <span class="math inline">\(\mu\)</span>, meaning <span class="math inline">\(p(\mu, y)=f(\mu-y)\)</span>. The likelihood <span class="math inline">\(L_{\vec{y}}(\mu)=\prod_i f(\mu-y_i)\)</span>.</p>
<p>Suppose we have, for all <span class="math inline">\(\vec{y}\)</span> near <span class="math inline">\(\vec{y}_0\)</span>, an isolated local minimum of <span class="math inline">\(L_{\vec{y}}\)</span> at <span class="math inline">\(\hat{\mu}(\vec{y})\)</span>. Suppose further we can write <span class="math inline">\(\hat{\mu}(\vec{y}) = \sum y_i w_i(\vec{y})\)</span> with <span class="math inline">\(w_i\)</span> (continuous) functions of the differences <span class="math inline">\(y_k-y_l\)</span>. Shift each <span class="math inline">\(y_i\)</span> by a small <span class="math inline">\(\delta\)</span>, to <span class="math inline">\(\tilde{y}_i=y_i+\delta\)</span>. Then, since <span class="math inline">\(\mu\)</span> is a location parameter, likelihood is also shifted: <span class="math inline">\(L_{\tilde{\vec{y}}}(\mu+\delta)=L_{\vec{y}}(\mu)\)</span> so</p>
<p><span class="math display">\[\hat{\mu}(\tilde{\vec{y}})=\hat{\mu}(\vec{y})+\delta.\]</span></p>
<p>On other hand we have <span class="math inline">\(\hat{\mu}(\vec{y})=\sum y_i w_i(\vec{y})\)</span> and since all <span class="math inline">\(w_i(\vec{y})\)</span> are unchanged by the shift plugging in <span class="math inline">\(\tilde{y}\)</span> we get</p>
<p><span class="math display">\[\hat{\mu}(\tilde{\vec{y}})=\sum (y_i+\delta)w_i=\hat{\mu}(y)+\delta(\sum w_i(\vec{y})).\]</span></p>
<p>We conclude <span class="math inline">\(\sum_i w_i(\vec{y})=1\)</span>.</p>
<p><strong>Is MLE for location parameter a weighted average? Cases <span class="math inline">\(n=1\)</span> and <span class="math inline">\(n=2\)</span>.</strong></p>
<p>Now we look at wether <span class="math inline">\(\hat{\mu}(\vec{y}) = \sum y_i w_i(\vec{y})\)</span> is indeed true.</p>
<p>It is instructive to examine the case <span class="math inline">\(n=1\)</span>. Then we are just looking at local optima of <span class="math inline">\(f(\mu-y_1)\)</span>. If <span class="math inline">\(f\)</span> is unimodular with unique maximum at <span class="math inline">\(0\)</span> then <span class="math inline">\(\hat{mu}=y\)</span> and of course <span class="math inline">\(w\)</span> is - as prescribed - function of no inputs that “sums” to 1, i.e. is the constant 1. For general <span class="math inline">\(f\)</span> we have various local maxima <span class="math inline">\(m_j\)</span> and MLE <span class="math inline">\(\hat{\mu}_j=y+m_j\)</span> which are not in the required form. It may be prudent to assume unimodality and peak at <span class="math inline">\(0\)</span> for this problem.</p>
<p>For <span class="math inline">\(n=2\)</span> such a form <span class="math inline">\(\hat{\mu}(y_1, y_2)= y_1 w_1(y_1-y_2)+y_2 w_2(y_1-y_2)\)</span> is achievable as follows. Denote <span class="math inline">\(\frac{y_1-y_2}{2}\)</span> by <span class="math inline">\(y\)</span>. If <span class="math inline">\(y=0\)</span> the two data points coincide and <span class="math inline">\(L_{\vec{y}}(\mu)=f(\mu-y_1)^2\)</span> which has the same minima as <span class="math inline">\(f(\mu-y_1)\)</span> - we effectively have only one data point, a case which we have examined above. So below we assume <span class="math inline">\(y\neq 0\)</span>.</p>
<p>Then by translating by <span class="math inline">\(\delta=\frac{y_1+y_2}{2}\)</span> we have <span class="math inline">\(\hat{\mu}(y_1, y_2)=\frac{y_1+y_2}{2}+\hat{\mu}(-y, y)\)</span>. Write <span class="math inline">\(\hat{\mu}(-y, y)=y w(y)\)</span>. Then</p>
<p><span class="math display">\[\hat{\mu}(y_1, y_2)=\frac{y_1+y_2}{2}+\hat{\mu}(-y, y)\]</span></p>
<p><span class="math display">\[=\frac{y_1+y_2}{2}+\frac{y_1-y_2}{2} w(y_1-y_2)=\]</span></p>
<p><span class="math display">\[y_1(w_1(y_1-y_2)) +y_2 w_2(y_1-y_2)\]</span></p>
<p>where</p>
<p><span class="math display">\[w_1(y_1-y_2)=\frac{1}{2}+\frac{1}{2}w(y_1-y_2)\]</span> <span class="math display">\[w_2(y_1-y_2)=\frac{1}{2}-\frac{1}{2}w(y_1-y_2).\]</span></p>
<p><strong>Cauchy distribution, mostly <span class="math inline">\(n=2\)</span>.</strong></p>
<p>We illustrate this in the case when the sampling distribution is Cauchy: <span class="math inline">\(p(y|\mu)=\frac{1}{\pi}\frac{1}{1+(y-\mu)^2}\)</span>.</p>
<p>For general <span class="math inline">\(n\)</span> and the sample <span class="math inline">\(y_1, \ldots, y_n\)</span> the likelihood is <span class="math inline">\(\prod_{i=1}^n \frac{1}{\pi}\frac{1}{1+(y_i-\mu)^2}\)</span>, and log likeliehood is up to a constant <span class="math inline">\(L_{\vec{y}}(\mu)=\sum_i -\ln(1+(y_i-\mu)^2)\)</span>. The extremality condition is <span class="math inline">\(\frac{d}{d\mu}L_{\vec{y}}(\mu)=0\)</span> i.e. <span class="math inline">\(\sum_i^n \frac{y_i-\mu}{1+(y_i-\mu)^2}=0.\)</span> This is in general equivalent to a degree <span class="math inline">\(2n-1\)</span> polynomial equation in <span class="math inline">\(\mu\)</span> - there are many local optima for the likelihood.</p>
<p>Consider now the case <span class="math inline">\(n=2\)</span>.</p>
<p>As before, we write <span class="math inline">\(y=\frac{y_1-y_2}{2}\)</span> consider the problem shifted by <span class="math inline">\(\delta=\frac{y_1+y_2}{2}\)</span>.</p>
<p>The optimality equation becomes <span class="math inline">\(\frac{y-\hat{\mu}}{1+(y-\hat{\mu})^2}-\frac{y+\hat{\mu}}{1+(y+\hat{\mu})^2}=0\)</span> so <span class="math inline">\(f(x)=\frac{x}{1+x^2}=\frac{1}{x+\frac{1}{x}}\)</span> has <span class="math inline">\(f(y-\hat{\mu})=f(y+\hat{\mu})\)</span>. Either <span class="math inline">\(y-\hat{\mu}=y+\hat{\mu}\)</span>, i.e. <span class="math inline">\(\hat{\mu}=0\)</span> or <span class="math inline">\((y-\hat{\mu})(y+\hat{\mu})=1\)</span>, <span class="math inline">\(\hat{\mu}=\pm\sqrt{y^2-1}\)</span>. This last pair of solution is real only if <span class="math inline">\(|y|&gt;1\)</span>.</p>
<p>Suppose that in fact <span class="math inline">\(|y|&gt;1\)</span>. Then one over the likelihood is a positive fourth degree polynomial which we now know has three local extrema - <span class="math inline">\(-\sqrt{y^2-1}, 0, \sqrt{y^2-1}\)</span>. Therefore these extrema must be non-degenerate and be min, max, min. Correspondingly, the (log)likelihood extrema must be max, min, max.</p>
<p>The MLE estimate is thus indifferent between <span class="math display">\[\hat{\mu}_1=\frac{y_1+y_2}{2}+\sqrt{(\frac{y_1-y_2}{2})^2-1}\]</span> and</p>
<p><span class="math display">\[\hat{\mu}_2=\frac{y_1+y_2}{2}-\sqrt{(\frac{y_1-y_2}{2})^2-1}.\]</span></p>
<p>Let’s see how this can be written in the form <span class="math inline">\(\hat{\mu}=y_1w_1(y_1-y_2)+y_2w_2(y_1-y_2)\)</span>. We treat <span class="math inline">\(\hat{\mu}_1\)</span></p>
<p>As prescribed,</p>
<p><span class="math display">\[w(y)=\sqrt{y^2-1}=y\cdot sgn(y) \sqrt{\frac{1}{4}-\frac{1}{y^2}}\]</span></p>
<p>so</p>
<p><span class="math display">\[w_1(y)=\frac{1}{2}+ sgn(y) \sqrt{\frac{1}{4}-\frac{1}{y^2}}\]</span></p>
<p>and</p>
<p><span class="math display">\[w_2(y)=\frac{1}{2}- sgn(y) \sqrt{\frac{1}{4}-\frac{1}{y^2}}\]</span></p>
<p>and indeed,</p>
<p><span class="math display">\[y_1w_1(y_1-y_2)+y_2w_2(y_1-y_2)=\]</span></p>
<p><span class="math display">\[\frac{y_1+y_2}{2}+(y_1-y_2)\cdot sgn(y_1-y_2) \sqrt{\frac{1}{4}-\frac{1}{(y_1-y_2)^2}}=\]</span></p>
<p><span class="math display">\[\frac{y_1+y_2}{2}+\sqrt{(\frac{y_1-y_2}{2})^2-1}=\]</span></p>
<p><span class="math display">\[y_1w_1(y_1-y_2)+y_2w_2(y_1-y_2)= \hat{\mu}_1\]</span></p>
<p>Observe that <span class="math inline">\(w_1(y)+w_2(y)=1\)</span> as expected. Observe also that for <span class="math inline">\(\hat{\mu}_2\)</span> we would obtain <span class="math inline">\(\tilde{w}_1=w_2\)</span> and <span class="math inline">\(\tilde{w}_2=w_1\)</span> - the individual <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(w_i\)</span> are not symmetric in exchanging <span class="math inline">\(y_i\)</span>s, but the sets of <span class="math inline">\(\mu\)</span>s and <span class="math inline">\(w\)</span>s are.</p>
<p><strong>Incomplete remarks on <span class="math inline">\(n&gt;2\)</span>.</strong></p>
<p>Still considering Cauchy distribution, but for general <span class="math inline">\(n\)</span>, the inverse of likelihood is a polynomial in <span class="math inline">\(\mu\)</span> of degree <span class="math inline">\(2n\)</span> with coefficients some symmetric polynomials in <span class="math inline">\(y_i\)</span>, and its derivative is a polynomial in <span class="math inline">\(\mu\)</span> of degree <span class="math inline">\(2n-1\)</span> with coefficients some symmetric polynomials in <span class="math inline">\(y_i\)</span>, which we can write as <span class="math inline">\(Q(\mu, y_1, \ldots, y_n)\)</span>. The hypersurface <span class="math inline">\(Q(\mu, y_1, \ldots, y_n)=Q(\mu, \vec{y})=0\)</span> in <span class="math inline">\(\mathbb{R}^{n+1}\)</span> projects to <span class="math inline">\(\vec{y}=y_1, \ldots, y_n\)</span>. In the <span class="math inline">\(\vec{y}\)</span> space and if we exclude those <span class="math inline">\(\vec{y}\)</span> tuples for which the discriminant of <span class="math inline">\(Q\)</span> is zero we have over <span class="math inline">\(\mathbb{R}^n\setminus D\)</span> a covering map, with varying number of sheets <span class="math inline">\(k\)</span>, being the number of soultions of <span class="math inline">\(Q(\mu, \vec{y})=0\)</span> in <span class="math inline">\(\mu\)</span> for given <span class="math inline">\(\vec{k}\)</span>. Locally in <span class="math inline">\(\mathbb{R}\setminus D\)</span>, there are <span class="math inline">\(k\)</span> “inverse of projection” maps <span class="math inline">\(M:C\to \mathbb{R}\)</span> that pick out a particular solution out of the <span class="math inline">\(k\)</span> in a continuous (and in fact smooth) way (a solution of <span class="math inline">\(Q(\mu, \vec{y})=0\)</span> is a local extremum of the likelihood function). Our task is to show that at least those <span class="math inline">\(M\)</span> that correspond to maxima of likeliehood can be written as <span class="math inline">\(M(\vec{y})=\sum_i y_i w_i(\vec{d})\)</span> where <span class="math inline">\(\vec{d}\)</span> is the vector of pairwise differences <span class="math inline">\(y_k-y_l\)</span> and <span class="math inline">\(\sum w_i(\vec{d})=1\)</span>. (Similar description applies to other distributions, except everything is no longer necessarily polynomial.) I have failed to prove this or find it in the literature.</p>
<!---I suspect considering the action of the permutation group on $y_i$ would be helpful;
---->
<p>## Exercise 7.4</p>
<p>We are minimizing <span class="math inline">\(\vec{w}^T C \vec{w}\)</span>. Let’s minimize over the hyperplane <span class="math inline">\(\sum w_i=1\)</span>. Since <span class="math inline">\(C\)</span> is positive definite on <span class="math inline">\(\mathbb{R}^n\)</span>, at infinity the values are large and positive. So the minimum is achieved at finite distance and must satisfy the Lagrange multiplier equation is <span class="math inline">\(C\vec{w}=\lambda \vec{1}\)</span>, so <span class="math inline">\(\vec{w}=\lambda C^{-1}\vec{1}\)</span> and <span class="math inline">\(\sum w_i=1\)</span> gives, with <span class="math inline">\(C^{-1}=K\)</span> the answer <span class="math inline">\(w_i=\sum_j K_{ij}/\sum_{i,j}K_{ij}\)</span>, as wanted (note that the denominator is <span class="math inline">\(\vec{1}^T K \vec{1}&gt;0\)</span>).</p>
<p>Corresponding value is</p>
<p><span class="math display">\[\vec{w}^T C \vec{w}=\lambda^2 (C^{-1}\vec{1})^T C (C^{-1}\vec{1})=\lambda=(\sum_{ij} K_{ij})^{-1}\]</span></p>
<p>HOWEVER this answer satisfies does not always satisfy the constraints <span class="math inline">\(w_i\geq 0\)</span>: consider <span class="math inline">\(C=\begin{pmatrix}1 &amp; 4\\ 4&amp; 17\end{pmatrix}\)</span> so that <span class="math inline">\(K=\begin{pmatrix}17 &amp; -4\\ -4&amp; 1\end{pmatrix}\)</span>; then <span class="math inline">\(w=(1.3, -0.3)^T\)</span>.</p>
<h2 id="exercise-7.5">Exercise 7.5</h2>
<p>See https://www.cs.toronto.edu/~yuvalf/CLT.pdf</p>
<h2 id="section">7.84</h2>
<p><span class="math display">\[\exp\{xa-\frac{a^2}{2}\}=\exp\{\frac{x^2}{2}\}\exp\{-\frac{(x-a)^2}{2}\}\]</span></p>
<p>so</p>
<p><span class="math display">\[\frac{d^n}{da^n} \exp\{xa-\frac{a^2}{2}\}=\exp\{\frac{x^2}{2}\} \frac{d^n}{da^n}\left(\exp\{-\frac{(x-a)^2}{2}\}\right)\]</span></p>
<h2 id="section-1">7.85</h2>
<p><span class="math display">\[\frac{\phi(x-a)\phi(x-b)}{\phi(x)}= \phi(x)\left(\sum_n R_n(x)\frac{a^n}{n!}\right)\left(\sum_m R_m(x)\frac{b^m}{m!}\right)\]</span></p>
<p>LHS: Observe <span class="math display">\[\phi(x-a)\phi(x-b)= \phi(x)\phi(x-(a+b)) \exp\{ab\}\]</span></p>
<p>Then</p>
<p><span class="math display">\[ \int \frac{\phi(x-a)\phi(x-b)}{\phi(x)} dx=\int \phi(x-(a+b)) \exp\{ab\} dx=\exp\{ab\}\]</span></p>
<p>As a power series in <span class="math inline">\(ab\)</span> it is <span class="math inline">\(\sum_i \frac{a^ib^i}{i!}\)</span>.</p>
<p>RHS:</p>
<p><span class="math inline">\(\sum_{n,m} (\int \phi(x) R_n(x)R_m(x)dx) \frac{a^n b^m}{n! m!}\)</span></p>
<p>Equating coefficients we get 7.85.</p>
<h2 id="section-2">7.86</h2>
<p>From 7.83 <span class="math inline">\(\phi(x-y)=\phi(x)\sum_m R_m(x)\frac{y^m}{m!}\)</span> so</p>
<p><span class="math display">\[\phi(x-y)R_n(x)=R_n(x)\phi(x)\sum_m R_m(x)\frac{y^m}{m!}\]</span></p>
<p>and integrating and using 7.85 we get</p>
<p><span class="math display">\[\int \phi(x-y) R_n(x)dx= y^n.\]</span></p>
<h2 id="section-3">7.89</h2>
<p><span class="math display">\[\exp\{xa\}\exp\{-a^2/2\}=\sum_k \frac{x^k a^k}{k!}\sum_m \frac{a^{2m}}{(-2)^m m!}\]</span></p>
<p>Isolating the term in front of <span class="math inline">\(n=k+2m\)</span> gives 7.89</p>
</body>
</html>
