<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>chapter3</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="elementary-sampling-theory">Elementary Sampling theory</h1>
<p><span class="math inline">\(\leftarrow\)</span> <a href="./index.html">Back to Chapters</a></p>
<h2 id="the-most-probable-value-of-r">3.26, 3.27 The most probable value of <span class="math inline">\(r\)</span></h2>
<p>The sequence <span class="math inline">\(h(r|N, M, n)\)</span> for fixed <span class="math inline">\(N, M, n\)</span> is unimodal, meaning it first increases, then decreases. To see this we argue as follows.</p>
<p>We want to see wether <span class="math inline">\(h(r+1|N, M, n)\)</span> is bigger than <span class="math inline">\(h(r|N, M, n)\)</span>, so we need to compare their fraction to <span class="math inline">\(1\)</span>. We compute using 3.22</p>
<p><span class="math display">\[h(r+1|N, M, n)/h(r|N, M, n)=\]</span></p>
<p><span class="math display">\[ \frac{(M-r) /(r+1)}{(N-M - n+ r+1) /(n-r)}=\]</span></p>
<p><span class="math display">\[ \frac{(r-M) (r-n)}{(r+1)(r+ N-M-n+1 )  }  \stackrel{?}{\gtreqless} 1\]</span></p>
<p><span class="math display">\[(r-M) (r-n)\stackrel{?}{\gtreqless}(r+1)(r+ N-M-n+1 ) \]</span></p>
<p><span class="math display">\[ Mn-(M+n)r  \stackrel{?}{\gtreqless} (N-M-n+1) +r (N-M-n+2)\]</span></p>
<p><span class="math display">\[ Mn-(N-M-n+1) \stackrel{?}{\gtreqless}  r (N+2)\]</span></p>
<p><span class="math display">\[\frac{ Mn-(N-M-n+1)}{N+2} \stackrel{?}{\gtreqless}  r \]</span></p>
<p><span class="math display">\[\frac{ Mn+M+n+1}{N+2} \stackrel{?}{\gtreqless}  r+1 \]</span></p>
<p><span class="math display">\[\frac{ (M+1)(n+1)}{N+2} \stackrel{?}{\gtreqless}  r+1 \]</span></p>
<p>The sequence <span class="math inline">\(h(r)\)</span> increases while the left hand side is bigger.</p>
<p>Thus denoting by <span class="math inline">\(r&#39;\)</span> the number <span class="math inline">\(\frac{ (M+1)(n+1)}{N+2}\)</span> we see that if <span class="math inline">\(r&#39;\)</span> is an integer, then <span class="math inline">\(h(r)\)</span> increase until <span class="math inline">\(r=r&#39;-1\)</span>, then <span class="math inline">\(h(r&#39;-1)=h(r&#39;)\)</span>, then the <span class="math inline">\(h(r)\)</span> decrease. If <span class="math inline">\(r&#39;\)</span> is not an integer, then <span class="math inline">\(h(r)\)</span> increase until <span class="math inline">\(h(INT(r&#39;))\)</span>, then decrease.</p>
<p>Remak 1: Note that the expected number of red balls is just the “naive” <span class="math inline">\(n \frac{M}{N}\)</span> (this is not hard to show using linearity of expectation, see Example 4.2.3 in Blitzstein-Hwang “Introduction to Probability”).</p>
<p>Remark 2: The above result can be restated in the following way: pretend to add one red and one white ball to the urn (for a total of <span class="math inline">\(N+2\)</span>) and draw <span class="math inline">\(n+1\)</span> balls from the resulting urn. Compute the “naive” most likely fraction of red balls <span class="math inline">\(\frac{M+1}{N+2}\)</span> and the “naive” most likely number of red balls <span class="math inline">\(\frac{(n+1)(M+1)}{N+2}\)</span>. Now subtract <span class="math inline">\(1\)</span>. This is (up to rounding) the most likely number of red balls drawn in the original procedure. This seems somewhat reminiscent of the correction that putting a beta prior on Bernoulli makes to the posterior expectation, but I have no idea if there is more to this connection than that.</p>
<h2 id="symmetry-of-hrn-m-n">3.29 Symmetry of <span class="math inline">\(h(r|N, M, n)\)</span></h2>
<p>Combinatorial proof that <span class="math display">\[h(r|N, M, n)=h(r|N, n, M).\]</span></p>
<p>Remark: This is Theorem 3.4.5 in Blitzstein - Hwang “Introduction to Probability”. See also Theorem 3.9.2.</p>
<p>By definition, <span class="math inline">\(h(r|N, M, n)\)</span> is computed as follows. Lay down <span class="math inline">\(N\)</span> balls, labelled <span class="math inline">\(1, ..., N\)</span>. Pick the subset <span class="math inline">\(R_0=\{1,..., M\}\)</span> of them and paint it red. Then pick a subset <span class="math inline">\(D\)</span> of size <span class="math inline">\(n\)</span> of all the ball, and compute <span class="math inline">\(r=|D\cap R_0|\)</span>. The fraction of <span class="math inline">\(D\)</span>s that give specific answer <span class="math inline">\(r\)</span> is by definition <span class="math inline">\(h(r|N, M, n)\)</span>.</p>
<p>Now suppose instead we pick a different subset <span class="math inline">\(R_1\)</span> of size <span class="math inline">\(M\)</span> to be red, and repeat the procedure above: pick <span class="math inline">\(D\)</span>, and compute <span class="math inline">\(r=|D \cap R_1|\)</span>. We claim that the fraction of <span class="math inline">\(D\)</span>s that give specific answer <span class="math inline">\(r\)</span> is still <span class="math inline">\(h(r|N, M, n)\)</span>. In deed, there exists a permutation of <span class="math inline">\(\{1, ..., N\}\)</span> taking <span class="math inline">\(R_1\)</span> to <span class="math inline">\(R_0\)</span> (“sort the reds to be first”); the same permutation takes <span class="math inline">\(D\)</span>s that give <span class="math inline">\(D\cap R_1=r\)</span> to those that give <span class="math inline">\(D\cap R_0=r\)</span>. Hence there are the same number of <span class="math inline">\(D\)</span>s in both circumstances.</p>
<p>The above argument means that <span class="math inline">\(h(r|N, M, n)\)</span> can be also computed as follows. Lay down <span class="math inline">\(N\)</span> balls, labeled <span class="math inline">\(1, ..., N\)</span>. Pick <strong>any</strong> subset <span class="math inline">\(R\)</span> of them of size <span class="math inline">\(M\)</span> and paint it red. Then pick a subset <span class="math inline">\(D\)</span> of size <span class="math inline">\(n\)</span> of all the ball, and compute <span class="math inline">\(r=|D\cap R|\)</span>. The fraction of <strong><span class="math inline">\(R\)</span>s and <span class="math inline">\(D\)</span>s</strong> that give specific answer <span class="math inline">\(r\)</span> is then <span class="math inline">\(h(r|N, M, n)\)</span>.</p>
<p>But the above procedure remains the same if we exchange <span class="math inline">\(M\)</span> and <span class="math inline">\(n\)</span> and rename “paint red” into “pick” and “pick” into “paint red”. Then it computes <span class="math inline">\(h(r|N, n, M)\)</span>. So the two numbers are equal.</p>
<p>Remark: This view of hypergeometric distribution as giving probabilities of overlap of two subsets (“the red” and “the picked”) removes all time dependence and, in my opinion, sheds a lot of light on the discussion at the end of Section 3.2.</p>
<h2 id="exercise-3.2">Exercise 3.2</h2>
<p>Modified from <a href="https://math.stackexchange.com/questions/1552250/e-t-jaynes-probability-theory-exercise-3-2">stackexchange</a>.</p>
<p>Let <span class="math inline">\(A_i\)</span> be the statement “color i was not drawn”. Then</p>
<p><span class="math display">\[P(A_i) = \frac{{N-N_i \choose m}}{{N\choose m}}\]</span></p>
<p>This is the number of ways of drawing <span class="math inline">\(m\)</span> balls from <span class="math inline">\(N-N_i\)</span> non-<span class="math inline">\(i\)</span> colored balls, divided by the number of ways of drawing <span class="math inline">\(m\)</span> balls from all <span class="math inline">\(N\)</span> colored balls. Similarly,</p>
<p><span class="math display">\[P(A_iA_j) = \frac{{N-N_i-Nj \choose m}}{{N\choose m}}, P(A_iA_jA_k) = \frac{{N-N_i-Nj-N_k \choose m}}{{N\choose m}} ...\]</span></p>
<p>The probability we want is <span class="math inline">\(1 - P(A_1+A_2+A_3+A_4+A_5)\)</span>, the probability that no colors will be missing from our draw.</p>
<p>By the sum rule this can be calculated as</p>
<p><span class="math display">\[1 -\sum\limits_{i} P(A_i) +  \sum\limits_{i&lt;j} P(A_iA_j) - \sum\limits_{i&lt;j&lt;k} P(A_iA_jA_k) ... \]</span></p>
<p>where the first sum term is over all subsets of 1 color, the second is over all subsets of 2 colors, etc.</p>
<p>This calculation can be done in python like so:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> itertools <span class="im">import</span> combinations</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> scipy.special <span class="im">import</span> comb, perm</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">def</span> prob_all_colors_drawn(m, N):</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="co">&#39;&#39;&#39; </span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">    m is number of balls drawn</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">    N is a list containing how many of each color is in the urn </span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">    &#39;&#39;&#39;</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>    k <span class="op">=</span> <span class="bu">len</span>(N) <span class="co"># number of colors</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>    </span>
<span id="cb1-11"><a href="#cb1-11"></a>    total <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># start with 1</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span class="co"># calculate each sum term</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>        conjunction_prob <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>        <span class="cf">for</span> Ns <span class="kw">in</span> combinations(N, i): <span class="co"># for all combinations of i colors</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>            conjunction_prob <span class="op">+=</span> comb(<span class="bu">sum</span>(N) <span class="op">-</span> <span class="bu">sum</span>(Ns), m,<span class="va">True</span>)<span class="op">/</span>comb(<span class="bu">sum</span>(N),m,<span class="va">True</span>)</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="co"># alternately add or subtract the sum term </span></span>
<span id="cb1-20"><a href="#cb1-20"></a>        total <span class="op">+=</span> ((<span class="op">-</span><span class="dv">1</span>)<span class="op">**</span>i)<span class="op">*</span>conjunction_prob</span>
<span id="cb1-21"><a href="#cb1-21"></a>    <span class="cf">return</span> total</span></code></pre></div>
<p>You can modify and run this code on <a href="https://colab.research.google.com/drive/1tkkGN7qkx3PzTtrOiNu0DY0EdcZp9p0F?usp=sharing">Google Colab</a>, and see a monte carlo approximation of the same problem.</p>
<p>The code shows that to be 90% confident of getting all 5 colors we need 15 draws.</p>
<h2 id="exercise-3.3">Exercise 3.3</h2>
<p>We can obtain an upper bound on <span class="math inline">\(p(k|colors=3)\)</span> like so: <span class="math display">\[
\begin{aligned}
p(k|colors=3) &amp;= \frac{\sum_{all N_1 ... N_k} p(colors=3|k, N_1, N_2, ... N_k)p(N_1, N_2, ... N_k|k)p(k)}{\sum_{k}p(colors=3|k)p(k)}\\
&amp;&lt; \sum_{all N_1 ... N_k} p(colors=3|k, N_1, N_2, ... N_k)p(N_1, N_2, ... N_k|k)\frac{1}{50}\times334\\
&amp;&lt; \max\limits_{N_1,...,N_k}[p(colors=3|k, N_1, N_2, ... N_k)]\times6.66\\
&amp;&lt; {k\choose3}\max\limits_{N_1,...,N_k}[p(\overline{A_1}\overline{A_2}\overline{A_3}A_4...A_k|k, N_1, N_2, ... N_k)]\times6.66\\
\end{aligned}
\]</span></p>
<p>We assume a uniform prior over k: <span class="math inline">\(p(k) = \frac{1}{50}\)</span>. The first step is taken finding a lower bound on the denominator. We know that <span class="math inline">\(p(colors=3|k=3) &gt; 0.15\)</span>, so the denominator must be greater than <span class="math inline">\(0.15/50 = 1/334\)</span>.</p>
<p>The second step is taken because it contains a weighted average. We can find an upper bound over the weighted average by finding the <span class="math inline">\(N_1, N_2, ... N_k\)</span> that maximises it.</p>
<p>The third step is found because the statement <span class="math inline">\(colors=3\)</span> is the logical sum of <span class="math inline">\(k\choose3\)</span> conjunctions of the form <span class="math inline">\(\overline{A_1}\overline{A_2}\overline{A_3}A_4...A_k\)</span>, each of which has 3 A’s negated. This sum is bounded by <span class="math inline">\({k\choose3}\max\limits_{N_1,...,N_k}[p(\overline{A_1}\overline{A_2}\overline{A_3}A_4...A_k|...)]\)</span>.</p>
<p><span class="math inline">\(p(\overline{A_1}\overline{A_2}\overline{A_3}A_4...A_k|k, N_1, N_2, ... N_k)\)</span> can be calculated with:</p>
<p><span class="math display">\[p(\overline{A_1}\overline{A_2}\overline{A_3}|A_4...A_k...)p(A_4...A_k|...) =[1-p(A_1+A_2+A_3|A_4...A_k...)]p(A_4...A_k|...)\]</span></p>
<p>From then it’s similar to the calculations from Exercise 3.2.</p>
<p>We run the calculations with the code in the same <a href="https://colab.research.google.com/drive/1tkkGN7qkx3PzTtrOiNu0DY0EdcZp9p0F?usp=sharing">Colab</a> as above, and show that we can be at least 99% confident that <span class="math inline">\(3 \leq k \leq 20\)</span>. I suspect the upper bound can be tightened significantly with weak assumptions.</p>
<h4 id="alternative-approach-data-likeliehood-estimates-no-prior.">Alternative approach: Data likeliehood estimates, no prior.</h4>
<p>Suppose the color counts in the bin are given by the tuple <span class="math inline">\(\vec{N}=(N_1, N_2, \ldots, N_k)\)</span> with <span class="math inline">\(N_i\geq 1\)</span>, <span class="math inline">\(\sum N_i=50\)</span> and <span class="math inline">\(k\geq 1\)</span>. Of course under this assumption the number of colors in the bin is just <span class="math inline">\(k\)</span>.</p>
<p>If we had a prior <span class="math inline">\(p(\vec{N})\)</span> for various <span class="math inline">\(\vec{N}\)</span> tuples, we would compute posterior over same tuples by multiplying the <span class="math inline">\(p(\vec{N})\)</span> by likeliehood of getting <span class="math inline">\(3\)</span> colors from a sample of 20 balls <span class="math inline">\(L(\vec{N})\)</span> (which is fully determined by <span class="math inline">\(\vec{N}\)</span>, see below), and renormalizing.</p>
<p>Let’s try to see what the data likeliehood would be <span class="math inline">\(L(\vec{N})\)</span> for different <span class="math inline">\(N_i\)</span> tuples (to see how much probability of each <span class="math inline">\(N_i\)</span> tuple is suppressed/boosted by the data).</p>
<p>So, again, we suppose the numbers of balls of different colors in the urn are <span class="math inline">\(N_1, N_2,\ldots, N_k\)</span>. What is the probability of event “a sample of 20 contains balls of exactly 3 colors”? First we choose the 3 colors, <span class="math inline">\(i_1\)</span>, <span class="math inline">\(i_2\)</span> and <span class="math inline">\(i_3\)</span> and then apply Exercise 3.2 to the triple <span class="math inline">\(N_{i_1}, N_{i_2}, N_{i_3}\)</span> to get</p>
<p><span class="math inline">\({N_{i_1}+ N_{i_2}+ N_{i_3} \choose 20}-{ N_{i_1}+ N_{i_2}\choose 20}-{ N_{i_2}+ N_{i_3}\choose 20}-{ N_{i_1}+ N_{i_3}\choose 20}+ { N_{i_1}\choose 20}+{ N_{i_2}\choose 20}+{ N_{i_3}\choose 20}\)</span></p>
<p>possible draws that satisfy this, summing over all the selections of <span class="math inline">\(i_1, i_2, i_3\)</span> to get the total number of draws with 3 colors. Each <span class="math inline">\(N_i\)</span> will be chosen in <span class="math inline">\(k-1\choose 2\)</span> triples and each pair <span class="math inline">\(N_i, N_j\)</span> in <span class="math inline">\(k-2\)</span> triples. So the sum is</p>
<p><span class="math inline">\(\sum_{triples} {N_{i_1}+ N_{i_2}+ N_{i_3} \choose 20} - (k-2) \sum_{pairs} {N_{j_1}+ N_{j_2}\choose 20}+ {k-1\choose 2 } \sum_l {N_l \choose 20}\)</span></p>
<p>(The total number of draws is always the same <span class="math inline">\({50 \choose 20}\)</span>.)</p>
<p>Now, <span class="math inline">\(Q(x)=20!{x\choose 20}= x(x-1)\ldots (x-19)\)</span> is increasing in <span class="math inline">\(x&gt;19\)</span>, with ratio <span class="math inline">\(Q(x+1)/Q(x)= x/(x-19)\)</span>; for <span class="math inline">\(x\)</span> near 50 this is a factor of about <span class="math inline">\(1.7\)</span>. So, at first glance, those <span class="math inline">\(k\)</span> tuples with largest possible <span class="math inline">\(i_1+i_2+i_3\)</span> will have highest data likeliehood. All those with <span class="math inline">\(i_1+i_2+i_3=50\)</span> have <span class="math inline">\(k=3\)</span> of course. The <span class="math inline">\(\vec{N}=[48, 1, 1]\)</span> has <span class="math inline">\(47\choose 17\)</span> sequences, and data likeliehood of about <span class="math inline">\(0.06\)</span>. The <span class="math inline">\(\vec{N}=[17,17,16]\)</span> has data likeliehood <span class="math inline">\(0.99995\)</span>.</p>
<p>The <span class="math inline">\(\vec{N}=[51-k, 1, \ldots, 1]\)</span> gives data likeliehood <span class="math inline">\({k-1\choose 2}{50-k \choose 17}\)</span>, which goes</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="odd">
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(5.82\cdot 10^{-2}\)</span></td>
<td><span class="math inline">\(1.11\cdot 10^{-1}\)</span></td>
<td><span class="math inline">\(1.40\cdot 10^{-1}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(1.46\cdot 10^{-1}\)</span></td>
<td><span class="math inline">\(1.34\cdot 10^{-1}\)</span></td>
<td><span class="math inline">\(1.13\cdot 10^{-1}\)</span></td>
<td><span class="math inline">\(9.01\cdot 10^{-2}\)</span></td>
<td><span class="math inline">\(6.78\cdot 10^{-2}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(5.20\cdot 10^{-3}\)</span></td>
<td><span class="math inline">\(2.97\cdot 10^{-3}\)</span></td>
<td><span class="math inline">\(1.63\cdot 10^{-3}\)</span></td>
<td><span class="math inline">\(8.61\cdot 10^{-4}\)</span></td>
<td><span class="math inline">\(4.35\cdot 10^{-4}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(2.20\cdot 10^{-6}\)</span></td>
<td><span class="math inline">\(6.96\cdot 10^{-7}\)</span></td>
<td><span class="math inline">\(1.96\cdot 10^{-7}\)</span></td>
<td><span class="math inline">\(4.80\cdot 10^{-8}\)</span></td>
<td><span class="math inline">\(9.82\cdot 10^{-9}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(1.58\cdot 10^{-9}\)</span></td>
<td><span class="math inline">\(1.78\cdot 10^{-10}\)</span></td>
<td><span class="math inline">\(1.05\cdot 10^{-11}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p>So when <span class="math inline">\(k\)</span> reaches 16 even the most advantageous color counts <span class="math inline">\(\vec{N}\)</span> are suppressed at least <span class="math inline">\(10^4\)</span> times more than the most disadvantageous ones with <span class="math inline">\(k=3\)</span>. So it seems no matter what reasonable prior for the color counts one takes the posterior should be mostly supported on <span class="math inline">\(3\leq k\leq 16.\)</span></p>
<p><strong>Remark</strong>: For exact inference, it is not sufficient to have a prior over <span class="math inline">\(k\)</span>, since same prior over <span class="math inline">\(k\)</span> may correspond to different priors over <span class="math inline">\(\vec{N}\)</span>, producing different posteriors (the reason being that data likeliehoods are not determined by <span class="math inline">\(k\)</span>). Thus a prior over <span class="math inline">\(\vec{N}\)</span> is needed. Of course, approximate inference a prior over <span class="math inline">\(k\)</span> may be sufficient (as in the first approach above; however it seems likely that a reasonable prior over <span class="math inline">\(\vec{N}\)</span> will not result in a uniform prior over <span class="math inline">\(k\)</span>).</p>
<p><strong>Discussion of priors</strong> How to get a prior over <span class="math inline">\(\vec{N}\)</span> is not clear to me. One option is to model the urn being filled by sampling from an (infinite ) population. We can use several versions:</p>
<p>Version 1: The population has <span class="math inline">\(K\)</span> colors, with frequencies <span class="math inline">\(p_1, \ldots, p_K\)</span>. The set of such populations is the union of <span class="math inline">\(K-1\)</span> dimensional simplexes for <span class="math inline">\(K=1, 2, \ldots\)</span>.</p>
<p>Version 2: The population has infinitely many colors and probabilities of each color <span class="math inline">\(p_i\)</span>. The set of such populations is the “infinite dimensional simplex” <span class="math inline">\(p_i\geq 0\)</span>, <span class="math inline">\(\sum p_i=1\)</span>.</p>
<p>For each population (of either kind), the probability of every <span class="math inline">\(\vec{N}\)</span> is determined. So if we had a prior over the population types it would determine a corresponding prior over fully specified (though maybe still intractable) inference problem.</p>
<p>How to get a prior over population types also seems unclear. One could try to take some maximal entropy priors, or do some further hierarchical modeling, but since I do not plan to actually implement the inference, I will not go into details of this.</p>
<h2 id="exercise-3.4">Exercise 3.4</h2>
<p>Denote by <span class="math inline">\(F_i\)</span> be the event “<span class="math inline">\(i\)</span> is fixed”, and, for any <span class="math inline">\(I \subset \{1, ..., n\}\)</span>, denote by <span class="math inline">\(F_I\)</span> the event “all <span class="math inline">\(i\)</span> in <span class="math inline">\(I\)</span> are fixed”, i.e. <span class="math inline">\(F_I=\prod_{i\in I} F_i\)</span>.</p>
<p>We are looking for <span class="math inline">\(P(\sum F_i)\)</span>. By inclusion exclusion this is</p>
<p><span class="math display">\[P(\sum F_i)=\sum_{I \subset \{1, ..., n\}, I\neq \emptyset} (-1)^{|I|+1}P(F_I).\]</span></p>
<p>For a given subset of size <span class="math inline">\(k\)</span> probability that it is fixed is <span class="math inline">\(\frac{(n-k)!}{n!}\)</span>, and there are <span class="math inline">\(n\choose k\)</span> such subsets, so the sum over those <span class="math inline">\(I\)</span> with size <span class="math inline">\(k\)</span> gives <span class="math inline">\((-1)^{k+1}\frac{1}{k!}\)</span>. Plugging this in we obtain</p>
<p><span class="math display">\[h=P(\sum F_i)=\sum_{k=1}^{n} (-1)^{k+1}\frac{1}{k!},\]</span> as wanted.</p>
<p>Observe that <span class="math inline">\(1-h\)</span> is the value of <span class="math inline">\(k\)</span>-th order Taylor series for <span class="math inline">\(e^x\)</span> evaluated at <span class="math inline">\(x=-1\)</span>, which, as <span class="math inline">\(k\to \infty\)</span>, converges to <span class="math inline">\(e^{-1}=1/e\)</span>.</p>
<h2 id="exercise-3.5">Exercise 3.5</h2>
<p>Similarly to 3.4, consider the event <span class="math inline">\(E_I\)</span>=the bins with labels <span class="math inline">\(i\in I\)</span> are left empty; then <span class="math inline">\(P(E_I)= (M-|I|)^N/M^N\)</span> and by inclusion-exclusion <span class="math inline">\(P(\overline{\sum E_i})\)</span> is <span class="math display">\[\frac{1}{M^N}\sum_{k=0}^{M} (-1)^{k} \binom{M}{k} (M-k)^N.\]</span></p>
<p>Remark: We are computing probability that a function from a set of size <span class="math inline">\(N\)</span> to a set of size <span class="math inline">\(M\)</span> is onto. There are <span class="math inline">\(M^N\)</span> total functions, and the number of surjective ones is <span class="math inline">\(M!\)</span> times a <a href="https://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind">Stirling number of second kind</a>.</p>
<h2 id="some-of-exercise-3.6">Some of Exercise 3.6</h2>
<p>Remark: If the initial distribution (for <span class="math inline">\(R_0\)</span>, i.e. <span class="math inline">\(P(red)=P(R_0)=p,\)</span> <span class="math inline">\(P(white)=P(\bar{R}_0)=q\)</span>) were the same as the limit distribution <span class="math inline">\(\pi\)</span> (formula 3.125, <span class="math inline">\(\pi(red)=\lim P(R_k)=\frac{p-\delta}{1-\epsilon-\delta},\)</span> <span class="math inline">\(\pi(white)=\lim P(\bar{R}_k)=\frac{q-\epsilon}{1-\epsilon-\delta}\)</span>}, this would be a steady state Markov chain, whose time-reverse process is also a Markov chain with transition probabilities <span class="math inline">\(M_{ij}^r=\frac{\pi_j}{\pi_i}M_{ji}\)</span> (note that for 2 state chains one always has <span class="math inline">\(M_{ij}^r=M_{ij}\)</span>). This is precisely condition 3.131. Under this condition it is easy to compute <span class="math inline">\(P(R_j|R_k)\)</span> with <span class="math inline">\(j&lt;k\)</span>, and in the 2-state case that we are considering, they would be the same as <span class="math inline">\(P(R_k|R_j)\)</span> (as in 3.134). However in this exercise the Markov chain starts from the initial distribution that, in general, is not the steady state distribution, so reversing the time produces a process (indexed by negative integers) which is a Markov chain which is not time-homogeneous. Maybe there is still a way to apply general theory of Markov chains to the problem of “backward inference” in this setting; absent that, we proceed by a direct computation (but observe that the reversed process is connected to the limiting behavior of the result, see below).</p>
<p>As usual, all probabilities are conditioned on <span class="math inline">\(C\)</span>. Equation 3.129 is</p>
<p><span class="math display">\[ P(R_k |R_j) P(R_j) = P(R_j |R_k) P(R_k) \]</span></p>
<p>Equation 3.118 is</p>
<p><span class="math display">\[P(R_k)=\frac{(p-\delta)+(\epsilon+\delta)^{k-1} (p \epsilon-q\delta)}{1-\epsilon-\delta}\]</span></p>
<p>Finally equation 3.128 is</p>
<p><span class="math display">\[P(R_k|R_j)=\frac{(p-\delta)+(\epsilon+\delta)^{k-j} (q- \epsilon)}{1-\epsilon-\delta}\]</span></p>
<p>Plugging in</p>
<p><span class="math display">\[P(R_j |R_k)=  \frac{(p-\delta)+(\epsilon+\delta)^{j-1} (p \epsilon-q\delta)} {(p-\delta)+(\epsilon+\delta)^{k-1} (p \epsilon-q\delta)} \frac{(p-\delta)+(\epsilon+\delta)^{k-j} (q- \epsilon)}{1-\epsilon-\delta}\]</span></p>
<p>If both <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> go to infinity but <span class="math inline">\(k-j\)</span> is kept constant, this converges to</p>
<p><span class="math display">\[P(R_{\infty+d} |R_{\infty})=\frac{(p-\delta)+(\epsilon+\delta)^{d} (q- \epsilon)}{1-\epsilon-\delta},\]</span></p>
<p>which is precisely the “reversed process” result (for large <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> the influence of the initial distribution not being the stationary one has dissipated; in the 2 state case the reversed process is the same as the forward one, but that’s a special feature; when the number of states (i.e. colors) is higher limit behavior of “backward inference” is given by the reversed process).</p>
</body>
</html>
