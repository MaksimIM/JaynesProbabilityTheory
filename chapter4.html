<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>chapter4</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="elementary-hypothesis-testing">Elementary hypothesis testing</h1>
<p><span class="math inline">\(\leftarrow\)</span> <a href="./index.html">Back to Chapters</a></p>
<h3 id="exercise-4.1">Exercise 4.1</h3>
<p>Given <span class="math display">\[P(D_1 ... D_m|H_i X)=\prod_j P(D_j|H_iX)\]</span></p>
<p>and</p>
<p><span class="math display">\[P(D_1 ... D_m|\overline{H}_i X)=\prod_j P(D_j|\overline{H}_iX)\]</span></p>
<p>for <span class="math inline">\(1\leq i\leq n\)</span> and <span class="math inline">\(n&gt;2\)</span> show that for any fixed <span class="math inline">\(i\)</span> at most one of</p>
<p><span class="math display">\[\frac{P(D_j|H_iX)}{P(D_j|\overline{H}_iX)}\]</span></p>
<p>is not equal to <span class="math inline">\(1.\)</span></p>
<p>Proof:</p>
<p>Firstly, we claim that the case of <span class="math inline">\(2\)</span> pieces of data implies the general result.</p>
<p>Indeed, independence assumptions of all <span class="math inline">\(D_j\)</span> together imply analogous pairwise independence for any pair <span class="math inline">\(D_k\)</span> and <span class="math inline">\(D_l\)</span>, and so, assuming the case with two data pieces is solved, for a fixed <span class="math inline">\(i\)</span> and for any pair of <span class="math inline">\(k, l\)</span> <span class="math inline">\(\frac{P(D_k|H_iX)}{P(D_k|\overline{H}_iX)}\)</span>, <span class="math inline">\(\frac{P(D_l|H_iX)}{P(D_l|\overline{H}_iX)}\)</span> at most one is not equal to <span class="math inline">\(1\)</span>, so of the whole set of <span class="math inline">\(\frac{P(D_j|H_iX)}{P(D_j|\overline{H}_iX)}\)</span> at most one is not equal to <span class="math inline">\(1.\)</span></p>
<p>So we focus on the case of only two data sets, <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span>.</p>
<p>We will denote probability density/mass function of <span class="math inline">\(D_1\)</span> under hypothesis <span class="math inline">\(H_iX\)</span> by <span class="math inline">\(V_i\)</span> and that of <span class="math inline">\(D_2\)</span> by <span class="math inline">\(U_i\)</span>. We will also denote probability density/mass function of <span class="math inline">\(D_1\)</span> under hypothesis <span class="math inline">\(\overline{H}_iX\)</span> by <span class="math inline">\(V^c_i\)</span> and that of <span class="math inline">\(D_2\)</span> by <span class="math inline">\(U^c_i\)</span>, though we will not use these until the very end.</p>
<p>Remark 1: The proof actually works for arbitrary (not necessarily discrete or continuous) real-valued random variables, one just has to say that <span class="math inline">\(V\)</span>s and <span class="math inline">\(U\)</span>s are CDFs instead of PMFs/PDFs.</p>
<p>We will denote value of <span class="math inline">\(V_i\)</span> at some <span class="math inline">\(D_1=x_1\)</span> by <span class="math inline">\(v_{i1}\)</span> (and at <span class="math inline">\(D_1=x_2\)</span> by <span class="math inline">\(v_{i2}\)</span>). Similarly the values of <span class="math inline">\(U\)</span> at <span class="math inline">\(D_2=y_1\)</span> will be denoted by <span class="math inline">\(u_{i1}\)</span>.</p>
<p>We will use independence of <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> conditional on various hypotheses to prove independence under other hypotheses. Independence always means <span class="math inline">\(P(D_1=x, D_2=y|H)=P(D_1=x|H) P(D_2=y|H)\)</span> and can be checked by checking for specific values.</p>
<p>With this in mind, we pick any pair of possible value pairs <span class="math inline">\(D_1=x_1, D_2=y_1\)</span> and <span class="math inline">\(D_1=x_2, D_2=y_2\)</span>, fixed from now on, and form vectors <span class="math inline">\(v_i=\begin{bmatrix}v_{i1} \\ v_{i2}\end{bmatrix}\)</span> and <span class="math inline">\(u_i=\begin{bmatrix}u_{i1} \\u_{i2}\end{bmatrix}\)</span> <!---$P(D_1=x_1|H_iX)=v^i_1$ and $P(D_1=x_2|H_iX)=v^i_2$, $P(D_2=y_1|H_iX)=u^i_1$ and $P(D_2=y_2|H_iX)=u^i_2$
---></p>
<p>Then independence of <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> (conditional on <span class="math inline">\(H_iX\)</span>) says that the joint distribution of <span class="math inline">\(P(D_1=x, D_2=y|H_i X)\)</span> is a product of distributions of <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> and is given (at <span class="math inline">\(x= x_1, x_2\)</span> and <span class="math inline">\(y=y_1, y_2\)</span>) by the matrix</p>
<p><span class="math display">\[M_i=v_i u_i^T=\begin{bmatrix}v_{i1} \\ v_{i2}\end{bmatrix} \begin{bmatrix}u_{i1} &amp; u_{i2}\end{bmatrix}=\]</span></p>
<p><span class="math display">\[=\begin{bmatrix}v_{i1}u_{i1}&amp; v_{i1}u_{i2}  \\v_{i2}u_{i1} &amp; v_{i2}u_{i2}\end{bmatrix}\]</span></p>
<p>It follows from this that the (joint) probability matrix of <span class="math inline">\(D_1D_2\)</span> ( again, at <span class="math inline">\(x= x_1, x_2\)</span> and <span class="math inline">\(y=y_1, y_2\)</span>) conditional on <span class="math inline">\(\overline{H_i}X\)</span> is obtained by taking all the matrices of <span class="math inline">\(H_j\)</span> with <span class="math inline">\(j\neq i\)</span> weighing them by (prior) probabilities <span class="math inline">\(h_j\)</span> of <span class="math inline">\(H_j\)</span> and adding them (and then dividing by the sum of the weights, but this is an overall normalizing factor which will not be important for us). That is, the matrix is proportional to</p>
<p><span class="math display">\[\sum_{j\neq i} h_j  M_i=\sum_{j\neq i} h_j v_j u_j^T=\]</span></p>
<p><span class="math display">\[\begin{bmatrix}\sum_{j\neq i} h_j v_{i1}u_{i1}&amp; \sum_{j\neq i} h_j v_{i1}u_{i2}  \\\sum_{j\neq i} h_j v_{i2} u_{i1} &amp; \sum_{j\neq i} h_j v_{i2} u_{i2}\end{bmatrix}\]</span></p>
<p>Now the assumption that <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> are independent conditional on <span class="math inline">\(\overline{H}_i\)</span> means this matrix is also a product of “marginal” matrices of <span class="math inline">\(D_1| \overline{H}_i X\)</span> and <span class="math inline">\(D_2|\overline{H}_i X\)</span>, i.e. is of rank 1. This means that it has determiant 0.</p>
<h4 id="three-hypothesis.">Three hypothesis.</h4>
<p>Let’s start with the case of only 3 hypothesis <span class="math inline">\(H_1, H_2,H_3\)</span>. Start with <span class="math inline">\(i=3\)</span>.</p>
<p>#####Lemma: A sum <span class="math inline">\(M\)</span> of two rank 1 matrices <span class="math inline">\(M=h_1 v_1 u^T_1 +h_2 v_2 u^T_2\)</span> can only be rank 1 if either <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are linearly dependent or <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are linearly dependent.</p>
<p>A “conceptual” proof is as follows: Consider the image of <span class="math inline">\(M\)</span>. Vectors <span class="math inline">\(M(u_1)\)</span> and <span class="math inline">\(M (u_2)\)</span> are both linear combinations of <span class="math inline">\(h_1 v_1\)</span> and <span class="math inline">\(h_2 v_2\)</span>, so, to get that the image of <span class="math inline">\(M\)</span> is the span of <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>, it is enough that the matrix of coefficients <span class="math inline">\(G=\begin{pmatrix} u_1^T u_1&amp; u_1^T u_2\\u_2^T u_1&amp; u_2^T u_2\end{pmatrix}\)</span> is invertible. But <span class="math inline">\(G\)</span> is the Grammian of <span class="math inline">\(u_1, u_2\)</span> and is invertible precisely when <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are linearly independent (its determinant is the square of the area of the parallelogram spanned by <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2,\)</span> as you can easily verify). In that case (of independent <span class="math inline">\(u\)</span>s), the rank of <span class="math inline">\(M\)</span> is the dimension of the span of <span class="math inline">\(v_1, v_2\)</span> and if <span class="math inline">\(v_1, v_2\)</span> were independent, it would be 2. So if rank of <span class="math inline">\(M\)</span> is below 2, then either <span class="math inline">\(u\)</span>s or <span class="math inline">\(v\)</span>s are dependent, as wanted.</p>
<p>Remark 2: Those familiar with tensors may realize that we use metric in which <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are orthonormal (that’s the inverse of <span class="math inline">\(G\)</span>) to “raise and index” and go from a bilinear form encoded by <span class="math inline">\(M\)</span> to a linear map, whose range is then the span of <span class="math inline">\(v\)</span>s.</p>
<p>Remark 3: Alternatively, for those who don’t like linear algebra, a computational proof of Lemma 1 is as follows: A (non-zero) 2 by 2 matrix has rank one when its determinant is zero. Writing this out in our case we get:</p>
<p><span class="math display">\[[h_1 v_{11} u_{11}+h_2 v_{21} u_{21}][h_1 v_{12} u_{12}+h_2 v_{22} u_{22}]=\]</span></p>
<p><span class="math display">\[[h_1 v_{11} u_{12}+h_2 v_{21} u_{22}][h_1 v_{12} u_{11}+h_2 v_{22} u_{21}]\]</span></p>
<p>Additively canceling <span class="math inline">\(h_1^2 v_{11}v_{12}u_{11}u_{12}\)</span> and <span class="math inline">\(h_2^2 v_{21}v_{22}u_{21}u_{22}\)</span> and then dividing by <span class="math inline">\(h_1h_2\)</span> we have</p>
<p><span class="math display">\[ v_{11} u_{11} v_{22} u_{22}+ v_{21} u_{21}v_{12} u_{12}= \]</span></p>
<p><span class="math display">\[ v_{11} u_{12} v_{22} u_{21}+ v_{21} u_{22}v_{12} u_{11} \]</span></p>
<p>or <span class="math display">\[(v_{11}v_{22}-v_{12}v_{21})(u_{11}u_{22}-u_{12}u_{21})=0,\]</span></p>
<p>so either <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are linearly dependent, or <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are.</p>
<p>Continuing with the case of three hypothesis, recall <span class="math inline">\(v_i\)</span> and <span class="math inline">\(u_i\)</span> were likeliehood/probability vectors of <span class="math inline">\(D_1\)</span> taking values <span class="math inline">\(x_1, x_2\)</span> and <span class="math inline">\(D_2\)</span> taking values <span class="math inline">\(y_1, y_2\)</span> (under hypothesis <span class="math inline">\(H_iX\)</span>).</p>
<p>Observe that a pair of non-zero functions <span class="math inline">\(V_1\)</span> and <span class="math inline">\(V_2\)</span> such that <span class="math inline">\((V_1(x_1), V_1(x_2))\)</span> is always proportional to <span class="math inline">\((V_2(x_1), V_2(x_2))\)</span> are “globally” proportional meaning <span class="math inline">\(V_1=kV_2\)</span> (take any <span class="math inline">\(x\)</span> with <span class="math inline">\(V_2(x)\neq 0\)</span> and make <span class="math inline">\(k= V_1(x)/V_2(x)\)</span>).</p>
<p>If distributions of <span class="math inline">\(D_1\)</span> under <span class="math inline">\(H_1X\)</span> and <span class="math inline">\(H_2X\)</span> are different, then they are also not proportional. By the previous paragraph, this implies that there will be two values <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> giving unproportional <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>. Then for arbitrary pair of values <span class="math inline">\(y_1, y_2\)</span> of <span class="math inline">\(D_2\)</span> the corresponding vectors <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are proportional, so, again, by the previous paragraph, the whole probability mass/density functions <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> of <span class="math inline">\(D_2\)</span> under <span class="math inline">\(H_1X\)</span> and <span class="math inline">\(H_2X\)</span> are proportional, ergo equal.</p>
<p>So either <span class="math inline">\(V_1=V_2\)</span> or <span class="math inline">\(U_1=U_2\)</span>.</p>
<p>Now from <span class="math inline">\(i=1\)</span> and <span class="math inline">\(i=2\)</span> we get that (either <span class="math inline">\(V_2=V_3\)</span> or <span class="math inline">\(U_2=U_3\)</span>) and (either <span class="math inline">\(V_1=V_3\)</span> or <span class="math inline">\(U_1=U_3\)</span>). Since we have 3 equalities and only two types of deistributions (<span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>), either the <span class="math inline">\(V\)</span>s are equal twice, and <span class="math inline">\(V_1=V_2=V_3\)</span>, or the <span class="math inline">\(U\)</span>s are (and <span class="math inline">\(U_1=U_2=U_3\)</span>). Correspondingly either <span class="math inline">\(D_1\)</span> has same distribution under all 3 hypothesis, and then <span class="math inline">\(\frac{P(D_1|H_iX)}{P(D_1|\overline{H}_iX)}\)</span> are all equal to <span class="math inline">\(1\)</span>, or <span class="math inline">\(D_2\)</span> does (and then all <span class="math inline">\(\frac{P(D_2|H_iX)}{P(D_2|\overline{H}_iX)}\)</span> are equal to <span class="math inline">\(1\)</span>). In either case, we get what we want.</p>
<p>This completes the case of 3 hypothesis.</p>
<h4 id="the-general-case.">The general case.</h4>
<p>To get the extension to more than <span class="math inline">\(3\)</span> hypothesis we use the following approach. As we mentioned before, a 2 by 2 matrix is or rank at most 1 if its determinant is zero. So we need some an efficient way of telling when the deterimnant of 2 by 2 matrix is zero.</p>
<p>Remark 4: More generally, a matrix is of rank at most 1 if all 2 by 2 minors have determinannt zero i.e. all <span class="math inline">\(M^{\wedge 2}_{(i,j), (k,l)}=M_{ik}M_{jl}-M_{il}M_{jk}\)</span> are zero. In tensor analysis, these are the entries of the second exterior power <span class="math inline">\(M^{\wedge 2}\)</span> of <span class="math inline">\(M\)</span>. When dimension is <span class="math inline">\(2\)</span> there is only one minor, and the <span class="math inline">\(M^{\wedge 2}\)</span> is a scalar, equal to <span class="math inline">\(\det M\)</span>. So in dimensions above 2, we can formulate everything that follows in terms of determinants.</p>
<p>We will use the following property of 2D determinants. If <span class="math inline">\(M\)</span> and <span class="math inline">\(N\)</span> are 2 by 2 matrices then</p>
<p><span class="math display">\[D(M, N) :=\frac{1}{2}(\det (M+N)-\det M -\det N)\]</span></p>
<p>is symmetric and bilinear in <span class="math inline">\(M, N\)</span>. This means</p>
<p><span class="math display">\[D(M, N)=D(N, M)\]</span></p>
<p>and</p>
<p><span class="math display">\[D(M_1+M_2, N)=D(M_1, N)+D(M_1, N)\]</span></p>
<p>(and hence the same for second variable). Indeed, one computes</p>
<p><span class="math inline">\(D(M, N)=M_{11}N_{22}+M_{22}N_{11}-M_{12}N_{21}-M_{21}N_{12}\)</span></p>
<p>and the resulting formula is linear in <span class="math inline">\(M\)</span> and in <span class="math inline">\(N\)</span>, i.e. bilinear.</p>
<p>Observe that <span class="math inline">\(D(M, M)=\det M\)</span>. We then have, by induction on the number of summands,</p>
<p><span class="math inline">\(\det (\sum M_i)=D(\sum M_i, \sum M_j)=\sum_{i, j} D(M_i, M_j)\)</span></p>
<p>Remark 5: We also have <span class="math inline">\(D(\lambda M, N)=\lambda D(M, N)\)</span>, as usual in bilinearity, but we don’t need this.</p>
<p>Remark 6: In higher dimensions, and using tensor language, we are saying that taking second exterior power, which is quadratic in the input, is a restriction of a symmetric bilinear operation (on two inputs).</p>
<!---Remrk 5: This is a little strange, but perhaps the following will fact can make slightly more palatable: if matrix $M$ a matrix encoding a PMF of a pair of binary random variables $X_1$ and $X_2$, and we assign value 0 to first outcome and 1 to the second one, then $\det M$ is the covariance $Cov(X_1, X_2)$
----->
<p>Now we can apply this to our problem. Let <span class="math inline">\(M_i=h_i v_i u_i^T\)</span> and <span class="math inline">\(N_i=\sum_{i\neq j} M_i\)</span>, and <span class="math inline">\(M=M_i+N_i=\sum_j M_j\)</span>.</p>
<p>Our assumptions are that all <span class="math inline">\(M_i\)</span> and <span class="math inline">\(N_j\)</span> are rank 1 (i.e. have zero determinant). We now show that <span class="math inline">\(M\)</span> has rank <span class="math inline">\(1\)</span> (i.e. has zero determinant).</p>
<p>To that end we write</p>
<p><span class="math display">\[\det M=\sum_{j, k} D(M_j, M_k)\]</span></p>
<p>We want to see that this is zero. We know</p>
<p><span class="math display">\[0=\det (N_i)=\sum_{j\neq i, k\neq i} D(M_j, M_k)\]</span></p>
<p>Summing over <span class="math inline">\(i\)</span> we get (taking note that each <span class="math inline">\(D(M_l, M_l)\)</span> will apear <span class="math inline">\(n-1\)</span> times, while those <span class="math inline">\(D(M_j, M_k)\)</span> with <span class="math inline">\(j\neq k\)</span> will appear only <span class="math inline">\(n-2\)</span> times):</p>
<p><span class="math display">\[\sum_l D(M_l, M_l) +  (n-2)\sum_{j,k} D(M_j, M_k)=0\]</span></p>
<p>So, since <span class="math inline">\(D(M_l, M_l)=0\)</span>, as long as <span class="math inline">\(n\neq 2\)</span> we have what we want.</p>
<p>This gives <span class="math inline">\(M=vu^T\)</span>. Going back to <span class="math inline">\(M=M_i+N_i\)</span> we again see two rank one matrices add up to a rank one matrix. We conclude, just as in the case of 3 hypothesis, that for each specific <span class="math inline">\(i\)</span>, either <span class="math inline">\(V^c_i=V_i\)</span> and hence <span class="math inline">\(\frac{P(D_1|H_iX)}{P(D_1|\overline{H}_iX)}=1\)</span></p>
<p>OR</p>
<p><span class="math inline">\(U^c_i=U_i\)</span> and hence <span class="math inline">\(\frac{P(D_2|H_iX)}{P(D_2|\overline{H}_iX)}=1\)</span>. This is exactly what we wanted to prove.</p>
</body>
</html>
