<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>chapter4</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="elementary-hypothesis-testing">Elementary hypothesis testing</h1>
<p><span class="math inline">\(\leftarrow\)</span> <a href="./index.html">Back to Chapters</a></p>
<h3 id="exercise-4.1">Exercise 4.1</h3>
<p>Given <span class="math display">\[P(D_1 ... D_m|H_i X)=\prod_j P(D_j|H_iX)\]</span></p>
<p>and</p>
<p><span class="math display">\[P(D_1 ... D_m|\overline{H}_i X)=\prod_j P(D_j|\overline{H}_iX)\]</span></p>
<p>for <span class="math inline">\(1\leq i\leq n\)</span> and <span class="math inline">\(n&gt;2\)</span> show that for any fixed <span class="math inline">\(i\)</span> at most one of</p>
<p><span class="math display">\[\frac{P(D_j|H_iX)}{P(D_j|\overline{H}_iX)}\]</span></p>
<p>is not equal to <span class="math inline">\(1\)</span>.</p>
<p>Proof:</p>
<p>Firstly, we claim that the case of <span class="math inline">\(2\)</span> pieces of data implies the general result.</p>
<p>Indeed, independence assumptions of all <span class="math inline">\(D_j\)</span> together imply analogous pairwise independence for any pair <span class="math inline">\(D_k\)</span> and <span class="math inline">\(D_l\)</span>, and so, assuming the case with two data pieces is solved, for any pair <span class="math inline">\(\frac{P(D_k|H_iX)}{P(D_k|\overline{H}_iX)}\)</span>, <span class="math inline">\(\frac{P(D_l|H_iX)}{P(D_l|\overline{H}_iX)}\)</span> at most one is not equal to <span class="math inline">\(1\)</span>, so of the whole set of <span class="math inline">\(\frac{P(D_j|H_iX)}{P(D_j|\overline{H}_iX)}\)</span> at most one is not equal to <span class="math inline">\(1\)</span>.</p>
<p>For a given hypothesis <span class="math inline">\(H_i\)</span> we let <span class="math inline">\(P(D_1|H_i X)=a_i\)</span> <span class="math inline">\(P(D_2|H_i X)=b_i\)</span>, so the ditribution of <span class="math inline">\(D_1\)</span> under <span class="math inline">\(H_iX\)</span> is given by a vector <span class="math inline">\(v_i= \begin{bmatrix}a_i \\ 1-a_i\end{bmatrix}\)</span> and that of <span class="math inline">\(D_2\)</span> by <span class="math inline">\(u_i= \begin{bmatrix}b_i \\ 1-b_i\end{bmatrix}\)</span></p>
<p>Then independence of <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> says that the joint distribution of <span class="math inline">\(D_1D_2\)</span> (conditional on <span class="math inline">\(H_iX\)</span>) is a product of distributions of <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> and is given by matrix</p>
<p><span class="math display">\[v_iu_i^T=\begin{bmatrix}a_i \\ 1-a_i\end{bmatrix}\begin{bmatrix}b_i &amp; 1-b_i\end{bmatrix}=\]</span></p>
<p><span class="math display">\[=\begin{bmatrix}a_ib_i&amp; a_i(1-b_i)  \\b_i(1-a_i) &amp; (1-a_i)(1-b_i)\end{bmatrix}\]</span></p>
<p>Then the joint probability matrix of <span class="math inline">\(D_1D_2\)</span> conditional on <span class="math inline">\(\overline{H_i}\)</span> is obtained by taking all the matrices of <span class="math inline">\(H_j\)</span> with <span class="math inline">\(j\neq i\)</span> weighing them by (prior) probabilities <span class="math inline">\(h_j\)</span> of <span class="math inline">\(H_j\)</span> and adding them (and then dividing by the sum of the weights, but this is an overall normalizing factor which will not be important for us). That is, the matrix is proportional to (all sums are over <span class="math inline">\(j\neq i\)</span>)</p>
<p><span class="math display">\[\sum_j h_j v_j u_j^T=\begin{bmatrix} \sum_j h_j a_jb_j&amp; \sum_j h_j  a_j(1-b_j)  \\ \sum_j h_j b_j(1-a_j) &amp; \sum_j h_j  (1-a_j)(1-b_j)\end{bmatrix}\]</span></p>
<p>Now the assumption that <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> are independent conditional on <span class="math inline">\(\overline{H}_i\)</span> means this matrix is also a product of marginal distributions of <span class="math inline">\(D_1| \overline{H}_i X\)</span> and <span class="math inline">\(D_2|\overline{H}_i X\)</span>, i.e. is of rank <span class="math inline">\(1\)</span>. This measn that it has determiant <span class="math inline">\(0\)</span>.</p>
<p>Let’s start with the case of just 3 hypothesis. Start with <span class="math inline">\(i=3\)</span>.</p>
<p>Then a “conceptual” proof is as follows:</p>
<p>A sum <span class="math inline">\(M\)</span> of two rank 1 matrices <span class="math inline">\(M=h_1 v_1 u^T_1 +h_2 v_2 u^T_2\)</span> can only be rank 1 if either <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are linearly dependent or <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are linearly dependent. Indeed, consider the image of <span class="math inline">\(M\)</span>. <span class="math inline">\(M(u_1)\)</span> and <span class="math inline">\(M (u_2)\)</span> are both linear combinations of <span class="math inline">\(h_1 v_1\)</span> and <span class="math inline">\(h_2 v_2\)</span>, so it is enough that the matrix of coefficients <span class="math inline">\(G=\begin{pmatrix} u_1^T u_1&amp; u_1^T\cdot u_2\\u_2^T u_1&amp; u_2^T\cdot u_2\end{pmatrix}\)</span> to get that the image of <span class="math inline">\(M\)</span> is span of <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>. But <span class="math inline">\(G\)</span> is the Grammian of <span class="math inline">\(u_1, u_2\)</span> and is invertible precisely when <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are linearly independent (its determinant is the square of the area of the parallelogram spanned by <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span>, as you can easily verify). So if that’s the case, then rank of <span class="math inline">\(M\)</span> is the dimension of the span of <span class="math inline">\(v_1, v_2\)</span> and is 2, not 1 if <span class="math inline">\(v_1, v_2\)</span> are independent, proving what we want.</p>
<p>Rmark: Those familiar with tensors may realize that we use metric in which <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are orthonormal (that’s the inverse of <span class="math inline">\(G\)</span>) to “raise and index” and go from a bilinear form encoded by <span class="math inline">\(M\)</span> to a linear map, whose range is then thespan of <span class="math inline">\(v\)</span>s.</p>
<p>Now if <span class="math inline">\(v_1\)</span> is linearly dependent with <span class="math inline">\(v_2\)</span> given that they are both probability vectors, this means <span class="math inline">\(v_1=v_2\)</span>, and similarly for <span class="math inline">\(u\)</span>s. So, either <span class="math inline">\(v_1=v_2\)</span> or <span class="math inline">\(u_1=u_2\)</span>.</p>
<p>Now from <span class="math inline">\(i=1\)</span> and <span class="math inline">\(i=2\)</span> we get that (either <span class="math inline">\(v_2=v_3\)</span> or <span class="math inline">\(u_2=u_3\)</span>) and (either <span class="math inline">\(v_1=v_3\)</span> or <span class="math inline">\(u_1=u_3\)</span>). Since we have 3 equalities and only two types of vectors, either <span class="math inline">\(v\)</span>s are equal twice, and <span class="math inline">\(v_1=v_2=v_3\)</span>, or <span class="math inline">\(u\)</span>s are (and <span class="math inline">\(u_1=u_2=u_3\)</span>). Correspondingly either <span class="math inline">\(D_1\)</span> has same distribution under all 3 hypothesis, and then <span class="math inline">\(\frac{P(D_1|H_iX)}{P(D_1|\overline{H}_iX)}\)</span> are all equal to <span class="math inline">\(1\)</span>, or <span class="math inline">\(D_2\)</span> does (and then all <span class="math inline">\(\frac{P(D_1|H_iX)}{P(D_1|\overline{H}_iX)}\)</span> are equal to <span class="math inline">\(1\)</span>). In either case, we get what we want.</p>
<p>Alternatively, for those who don’t like linear algebra computational proof is as follows:</p>
<p><span class="math display">\[[h_1 a_1 b_1+h_2 a_2 b_2][h_1 (1-a_1)(1-b_1)+h_2 (1-a_2) (1-b_2)]\]</span></p>
<p><span class="math display">\[=[h_1 a_1 (1-b_1)+h_2 a_2 (1-b_2)][h_1 (1-a_1)b_1+h_2 (1-a_2) b_2]\]</span></p>
<p>Additively canceling <span class="math inline">\(h_1^2 a_1(1-a_1)b_1(1-b_1)\)</span> and <span class="math inline">\(h_2^2 a_2(1-a_2)b_2(1-b_2)\)</span> and then dividing by <span class="math inline">\(h_1h_2(1-a_1)(1-a_2)(1-b_1)(1-b_2)\)</span> and denoting <span class="math inline">\(A_i=\frac{a_i}{1-a_i}\)</span> and <span class="math inline">\(B_i=\frac{b_i}{1-b_i}\)</span> we get</p>
<p><span class="math display">\[A_1B_1+A_2 B_2=A_1B_2+A_2B_1\]</span></p>
<p><span class="math display">\[(A_1-A_2)(B_1-B_2)=0\]</span></p>
<p>so either <span class="math inline">\(A_1=A_2\)</span> or <span class="math inline">\(B_1=B_2\)</span>. Observe that <span class="math inline">\(A_1=A_2\)</span> means <span class="math inline">\(a_1=a_2\)</span> (equal odds means equal probability).</p>
<p>From here on it’s the same: using <span class="math inline">\(i=2\)</span> we get either <span class="math inline">\(A_1=A_3\)</span> or <span class="math inline">\(B_1=B_3\)</span> and <span class="math inline">\(i=1\)</span> we get either <span class="math inline">\(A_2=A_3\)</span> or <span class="math inline">\(B_2=B_3\)</span>. Since there are 2 choices for <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> and 3 times this choice is made, we will either have <span class="math inline">\(A_1=A_2=A_3\)</span> or <span class="math inline">\(B_1=B_2=B_3\)</span>. In the first case <span class="math inline">\(D_1\)</span> is equally likely under all <span class="math inline">\(3\)</span> hypothesis, and so <span class="math inline">\(\frac{P(D_1|H_iX)}{P(D_1|\overline{H}_iX)}\)</span> are all equal to <span class="math inline">\(1\)</span>. In the other case all <span class="math inline">\(\frac{P(D_2|H_iX)}{P(D_2|\overline{H}_iX)}\)</span> are equal to <span class="math inline">\(1\)</span>.</p>
</body>
</html>
